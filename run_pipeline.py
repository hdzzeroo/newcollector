#!/usr/bin/env python
"""
é«˜æ€§èƒ½å¹¶è¡Œå¤„ç† Pipeline

æ”¯æŒé…ç½®:
- Chrome çˆ¬è™«è¿›ç¨‹æ•°
- Docling GPU è§£æè¿›ç¨‹æ•°
- LLM å¹¶å‘çº¿ç¨‹æ•°
- è‡ªåŠ¨èµ„æºæ£€æµ‹æ¨¡å¼ (--auto)

æ¶æ„:
Chrome Workers â†’ PDF Queue â†’ Docling GPU Workers â†’ LLM Thread Pool â†’ ç»“æœ
"""

import os
import sys
import time
import signal
import logging
import argparse
import subprocess
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime
from multiprocessing import Process, Queue, Manager, cpu_count
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Empty
from dotenv import load_dotenv

load_dotenv()


# ============================================================
# é…ç½®ï¼ˆå¿…é¡»åœ¨æœ€å‰é¢å®šä¹‰ï¼‰
# ============================================================

@dataclass
class PipelineConfig:
    """Pipeline é…ç½®"""
    # Chrome çˆ¬è™«é…ç½®
    chrome_workers: int = 4
    crawl_depth: int = 1

    # Docling GPU é…ç½®
    docling_workers: int = 3
    use_gpu: bool = True
    max_pages: int = 2

    # LLM é…ç½®
    llm_workers: int = 25

    # ä»»åŠ¡é…ç½®
    batch_size: int = 20
    link_type: Optional[str] = None
    max_batches: int = 0
    rest_time: int = 30

    # å…¶ä»–
    log_level: str = "INFO"


# ============================================================
# èµ„æºç›‘æ§å’Œè‡ªåŠ¨é…ç½®
# ============================================================

class ResourceMonitor:
    """èµ„æºç›‘æ§å™¨ - ç›‘æ§ CPUã€å†…å­˜ã€GPU ä½¿ç”¨æƒ…å†µ"""

    @staticmethod
    def get_cpu_info() -> Dict:
        """è·å– CPU ä¿¡æ¯"""
        try:
            import psutil
            return {
                'count': psutil.cpu_count(),
                'percent': psutil.cpu_percent(interval=1),
                'available': psutil.cpu_count() * (100 - psutil.cpu_percent()) / 100
            }
        except ImportError:
            # psutil æœªå®‰è£…ï¼Œä½¿ç”¨åŸºç¡€æ–¹æ³•
            return {
                'count': cpu_count(),
                'percent': 0,
                'available': cpu_count()
            }

    @staticmethod
    def get_memory_info() -> Dict:
        """è·å–å†…å­˜ä¿¡æ¯ (å•ä½: GB)"""
        try:
            import psutil
            mem = psutil.virtual_memory()
            return {
                'total': mem.total / (1024**3),
                'available': mem.available / (1024**3),
                'percent': mem.percent,
                'used': mem.used / (1024**3)
            }
        except ImportError:
            # å°è¯•ä» /proc/meminfo è¯»å–
            try:
                with open('/proc/meminfo', 'r') as f:
                    lines = f.readlines()
                    total = int(lines[0].split()[1]) / (1024**2)
                    available = int(lines[2].split()[1]) / (1024**2)
                    return {
                        'total': total,
                        'available': available,
                        'percent': (total - available) / total * 100,
                        'used': total - available
                    }
            except:
                return {'total': 64, 'available': 50, 'percent': 20, 'used': 14}

    @staticmethod
    def get_gpu_info() -> Dict:
        """è·å– GPU ä¿¡æ¯"""
        try:
            # ä½¿ç”¨ nvidia-smi å‘½ä»¤è·å– GPU ä¿¡æ¯
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=name,memory.total,memory.used,memory.free,utilization.gpu',
                 '--format=csv,noheader,nounits'],
                capture_output=True, text=True, timeout=10
            )

            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                gpus = []
                for line in lines:
                    parts = [p.strip() for p in line.split(',')]
                    if len(parts) >= 5:
                        gpus.append({
                            'name': parts[0],
                            'memory_total': float(parts[1]) / 1024,  # GB
                            'memory_used': float(parts[2]) / 1024,   # GB
                            'memory_free': float(parts[3]) / 1024,   # GB
                            'utilization': float(parts[4])
                        })

                if gpus:
                    gpu = gpus[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ª GPU
                    return {
                        'available': True,
                        'name': gpu['name'],
                        'memory_total': gpu['memory_total'],
                        'memory_free': gpu['memory_free'],
                        'memory_used': gpu['memory_used'],
                        'utilization': gpu['utilization'],
                        'count': len(gpus)
                    }
        except Exception as e:
            pass

        return {
            'available': False,
            'name': 'N/A',
            'memory_total': 0,
            'memory_free': 0,
            'memory_used': 0,
            'utilization': 0,
            'count': 0
        }

    @classmethod
    def get_all_resources(cls) -> Dict:
        """è·å–æ‰€æœ‰èµ„æºä¿¡æ¯"""
        return {
            'cpu': cls.get_cpu_info(),
            'memory': cls.get_memory_info(),
            'gpu': cls.get_gpu_info()
        }

    @classmethod
    def print_resources(cls):
        """æ‰“å°èµ„æºä¿¡æ¯"""
        res = cls.get_all_resources()

        print("\n" + "=" * 60)
        print("  ç³»ç»Ÿèµ„æºæ£€æµ‹")
        print("=" * 60)

        # CPU
        cpu = res['cpu']
        print(f"\n  CPU:")
        print(f"    æ ¸å¿ƒæ•°:     {cpu['count']}")
        print(f"    å½“å‰ä½¿ç”¨ç‡: {cpu['percent']:.1f}%")
        print(f"    å¯ç”¨æ ¸å¿ƒ:   {cpu['available']:.1f}")

        # å†…å­˜
        mem = res['memory']
        print(f"\n  å†…å­˜:")
        print(f"    æ€»é‡:       {mem['total']:.1f} GB")
        print(f"    å·²ä½¿ç”¨:     {mem['used']:.1f} GB ({mem['percent']:.1f}%)")
        print(f"    å¯ç”¨:       {mem['available']:.1f} GB")

        # GPU
        gpu = res['gpu']
        if gpu['available']:
            print(f"\n  GPU:")
            print(f"    å‹å·:       {gpu['name']}")
            print(f"    æ˜¾å­˜æ€»é‡:   {gpu['memory_total']:.1f} GB")
            print(f"    æ˜¾å­˜å·²ç”¨:   {gpu['memory_used']:.1f} GB")
            print(f"    æ˜¾å­˜å¯ç”¨:   {gpu['memory_free']:.1f} GB")
            print(f"    GPU ä½¿ç”¨ç‡: {gpu['utilization']:.1f}%")
        else:
            print(f"\n  GPU: æœªæ£€æµ‹åˆ°")

        print("=" * 60)
        return res


class AutoConfig:
    """è‡ªåŠ¨é…ç½®è®¡ç®—å™¨ - æ ¹æ®èµ„æºè‡ªåŠ¨è®¡ç®—æœ€ä¼˜é…ç½®"""

    # æ¯ä¸ªç»„ä»¶çš„èµ„æºæ¶ˆè€—ä¼°ç®—
    CHROME_MEMORY_GB = 1.5      # æ¯ä¸ª Chrome è¿›ç¨‹çº¦ 1.5GB å†…å­˜
    CHROME_CPU_CORES = 0.8     # æ¯ä¸ª Chrome è¿›ç¨‹çº¦ 0.8 CPU æ ¸å¿ƒ
    DOCLING_MEMORY_GB = 2.0    # æ¯ä¸ª Docling è¿›ç¨‹çº¦ 2GB å†…å­˜
    DOCLING_GPU_GB = 4.0       # æ¯ä¸ª Docling è¿›ç¨‹çº¦ 4GB æ˜¾å­˜
    DOCLING_CPU_CORES = 0.5    # æ¯ä¸ª Docling è¿›ç¨‹çº¦ 0.5 CPU æ ¸å¿ƒ

    # å®‰å…¨ä½™é‡ (ä¿ç•™ä¸€éƒ¨åˆ†èµ„æºç»™ç³»ç»Ÿ)
    MEMORY_SAFETY_MARGIN = 0.8   # ä½¿ç”¨ 80% çš„å¯ç”¨å†…å­˜
    GPU_SAFETY_MARGIN = 0.85     # ä½¿ç”¨ 85% çš„æ˜¾å­˜
    CPU_SAFETY_MARGIN = 0.9      # ä½¿ç”¨ 90% çš„ CPU

    @classmethod
    def calculate_optimal_config(cls, resources: Dict = None) -> PipelineConfig:
        """
        æ ¹æ®ç³»ç»Ÿèµ„æºè®¡ç®—æœ€ä¼˜é…ç½®

        Returns:
            PipelineConfig å¯¹è±¡
        """
        if resources is None:
            resources = ResourceMonitor.get_all_resources()

        cpu = resources['cpu']
        mem = resources['memory']
        gpu = resources['gpu']

        # å¯ç”¨èµ„æºï¼ˆè€ƒè™‘å®‰å…¨ä½™é‡ï¼‰
        available_memory = mem['available'] * cls.MEMORY_SAFETY_MARGIN
        available_cpu = cpu['available'] * cls.CPU_SAFETY_MARGIN
        available_gpu_memory = gpu['memory_free'] * cls.GPU_SAFETY_MARGIN if gpu['available'] else 0

        # è®¡ç®—å„ç»„ä»¶æœ€å¤§æ•°é‡

        # 1. Chrome workers (å—å†…å­˜å’Œ CPU é™åˆ¶)
        max_chrome_by_memory = int(available_memory / cls.CHROME_MEMORY_GB)
        max_chrome_by_cpu = int(available_cpu / cls.CHROME_CPU_CORES)
        chrome_workers = min(max_chrome_by_memory, max_chrome_by_cpu, 8)  # æœ€å¤š 8 ä¸ª
        chrome_workers = max(chrome_workers, 1)  # æœ€å°‘ 1 ä¸ª

        # æ›´æ–°å‰©ä½™èµ„æº
        remaining_memory = available_memory - (chrome_workers * cls.CHROME_MEMORY_GB)
        remaining_cpu = available_cpu - (chrome_workers * cls.CHROME_CPU_CORES)

        # 2. Docling workers (å—æ˜¾å­˜ã€å†…å­˜å’Œ CPU é™åˆ¶)
        if gpu['available'] and available_gpu_memory > cls.DOCLING_GPU_GB:
            max_docling_by_gpu = int(available_gpu_memory / cls.DOCLING_GPU_GB)
            max_docling_by_memory = int(remaining_memory / cls.DOCLING_MEMORY_GB)
            max_docling_by_cpu = int(remaining_cpu / cls.DOCLING_CPU_CORES)
            docling_workers = min(max_docling_by_gpu, max_docling_by_memory, max_docling_by_cpu, 6)
            docling_workers = max(docling_workers, 1)
            use_gpu = True
        else:
            # æ—  GPUï¼Œä½¿ç”¨ CPU æ¨¡å¼
            max_docling_by_memory = int(remaining_memory / cls.DOCLING_MEMORY_GB)
            max_docling_by_cpu = int(remaining_cpu / cls.DOCLING_CPU_CORES)
            docling_workers = min(max_docling_by_memory, max_docling_by_cpu, 4)
            docling_workers = max(docling_workers, 1)
            use_gpu = False

        # 3. LLM workers (I/O å¯†é›†ï¼Œä¸»è¦å— API é™åˆ¶)
        # ä¸€èˆ¬å»ºè®® 20-50 ä¸ªå¹¶å‘
        llm_workers = min(chrome_workers * 8, 50)
        llm_workers = max(llm_workers, 10)

        # 4. Batch size (åŸºäº Chrome workers)
        batch_size = chrome_workers * 5

        config = PipelineConfig(
            chrome_workers=chrome_workers,
            docling_workers=docling_workers,
            llm_workers=llm_workers,
            use_gpu=use_gpu,
            batch_size=batch_size,
            crawl_depth=1,
            max_pages=2,
            rest_time=30
        )

        return config

    @classmethod
    def print_recommendation(cls, config: PipelineConfig, resources: Dict):
        """æ‰“å°æ¨èé…ç½®"""
        mem = resources['memory']
        gpu = resources['gpu']

        print("\n" + "=" * 60)
        print("  è‡ªåŠ¨æ¨èé…ç½®")
        print("=" * 60)

        print(f"\n  Chrome Workers:  {config.chrome_workers}")
        print(f"    â””â”€ é¢„è®¡å†…å­˜å ç”¨: {config.chrome_workers * cls.CHROME_MEMORY_GB:.1f} GB")

        print(f"\n  Docling Workers: {config.docling_workers}")
        if config.use_gpu:
            print(f"    â””â”€ é¢„è®¡æ˜¾å­˜å ç”¨: {config.docling_workers * cls.DOCLING_GPU_GB:.1f} GB")
        print(f"    â””â”€ é¢„è®¡å†…å­˜å ç”¨: {config.docling_workers * cls.DOCLING_MEMORY_GB:.1f} GB")

        print(f"\n  LLM Workers:     {config.llm_workers}")
        print(f"    â””â”€ (I/O å¯†é›†å‹ï¼Œèµ„æºæ¶ˆè€—æå°)")

        print(f"\n  Batch Size:      {config.batch_size}")
        print(f"  Use GPU:         {'æ˜¯' if config.use_gpu else 'å¦'}")

        total_memory = (config.chrome_workers * cls.CHROME_MEMORY_GB +
                       config.docling_workers * cls.DOCLING_MEMORY_GB)
        print(f"\n  é¢„è®¡æ€»å†…å­˜å ç”¨:  {total_memory:.1f} GB / {mem['available']:.1f} GB å¯ç”¨")

        if config.use_gpu:
            total_gpu = config.docling_workers * cls.DOCLING_GPU_GB
            print(f"  é¢„è®¡æ€»æ˜¾å­˜å ç”¨:  {total_gpu:.1f} GB / {gpu['memory_free']:.1f} GB å¯ç”¨")

        print("=" * 60)


def auto_detect_config() -> Tuple[PipelineConfig, Dict]:
    """è‡ªåŠ¨æ£€æµ‹èµ„æºå¹¶è¿”å›æœ€ä¼˜é…ç½®"""
    resources = ResourceMonitor.print_resources()
    config = AutoConfig.calculate_optimal_config(resources)
    AutoConfig.print_recommendation(config, resources)
    return config, resources


# ============================================================
# æ—¥å¿—é…ç½®
# ============================================================

LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)

def setup_logging(level: str = "INFO"):
    """é…ç½®æ—¥å¿—"""
    log_file = os.path.join(LOG_DIR, f"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format='%(asctime)s [%(levelname)s] [%(processName)s] %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


# ============================================================
# æ•°æ®ç»“æ„
# ============================================================

@dataclass
class FileTask:
    """æ–‡ä»¶å¤„ç†ä»»åŠ¡"""
    task_id: int
    file_id: int
    local_path: str
    original_url: str
    original_name: str
    context: Dict[str, str]


@dataclass
class ExtractResult:
    """æå–ç»“æœ"""
    file_id: int
    task_id: int
    success: bool
    text: Optional[str] = None
    error_message: Optional[str] = None
    context: Optional[Dict] = None


# ============================================================
# Chrome çˆ¬è™« Worker
# ============================================================

def chrome_worker(
    worker_id: int,
    task_queue: Queue,
    file_queue: Queue,
    result_dict: Dict,
    config: Dict,
    stop_event
):
    """
    Chrome çˆ¬è™« Worker

    ä» task_queue è·å–é“¾æ¥ï¼Œçˆ¬å–åå°†æ–‡ä»¶æ”¾å…¥ file_queue
    """
    logger = logging.getLogger(f"Chrome-{worker_id}")
    logger.info(f"Chrome Worker {worker_id} å¯åŠ¨")

    # å¯¼å…¥ä¾èµ–
    from OverView import OverView, overViewInit
    from db.target_db import TargetDatabase
    from storage.downloader import FileDownloader
    from storage.supabase_storage import SupabaseStorage
    import Sdata

    chrome = None
    target_db = None

    try:
        # åˆå§‹åŒ– Chrome
        chrome = overViewInit()

        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        target_db = TargetDatabase()
        target_db.connect()

        # åˆå§‹åŒ–ä¸‹è½½å™¨å’Œå­˜å‚¨
        downloader = FileDownloader()
        storage = SupabaseStorage(is_public=False)
        storage.connect()

        while not stop_event.is_set():
            try:
                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ï¼ˆè¶…æ—¶ 5 ç§’ï¼‰
                link_data = task_queue.get(timeout=5)

                if link_data is None:  # åœæ­¢ä¿¡å·
                    logger.info(f"Chrome Worker {worker_id} æ”¶åˆ°åœæ­¢ä¿¡å·")
                    break

                link_id, link_url, link_type = link_data
                logger.info(f"[Worker-{worker_id}] å¼€å§‹å¤„ç† link_id={link_id}")

                # æ£€æŸ¥/åˆ›å»ºä»»åŠ¡
                existing_task = target_db.get_task_by_source_id(link_id)
                if existing_task:
                    task_id = existing_task.id
                else:
                    task_id = target_db.create_task(
                        source_link_id=link_id,
                        source_url=link_url,
                        school_name=None
                    )

                target_db.update_task_status(task_id, 'crawling')

                # çˆ¬å–
                sign = f"task_{task_id}"
                ov = OverView(link_url, depth=config['crawl_depth'], sign=sign)
                ov.SetOriUrl(link_url)
                ov.start(chrome)
                ov.Seek()
                ov.Pruning()

                # è·å–èŠ‚ç‚¹æ•°
                node_count = len(ov.URL_RLAB)

                # è¯»å–å‰ªæç»“æœ
                import csv
                cleaned_csv = ov.MemPath + "/" + Sdata.CSVCLEANED_FILENAME
                pruned_indices = []
                try:
                    with open(cleaned_csv, 'r', encoding='utf-8-sig') as f:
                        reader = csv.DictReader(f)
                        for row in reader:
                            pruned_indices.append(int(row['Index']))
                except Exception as e:
                    logger.warning(f"[Worker-{worker_id}] è¯»å–å‰ªæç»“æœå¤±è´¥: {e}")

                # ä¿å­˜èŠ‚ç‚¹åˆ°æ•°æ®åº“
                nodes_data = []
                for key in ov.URL_RLAB.keys():
                    node = ov.URL_RLAB[key]
                    nodes_data.append({
                        'Index': node[1],
                        'FatherIndex': node[2],
                        'Depth': node[3],
                        'title': node[4],
                        'Breadcrumb': node[5],
                        'Url': node[0],
                        'FatherTitle': ov.URL_RLAB[str(node[2])][4] if node[2] != -1 and str(node[2]) in ov.URL_RLAB else ""
                    })

                if nodes_data:
                    target_db.batch_insert_nodes(task_id, nodes_data)

                if pruned_indices:
                    target_db.mark_nodes_pruned(task_id, pruned_indices)

                ov.end()
                logger.info(f"[Worker-{worker_id}] çˆ¬å–å®Œæˆ, èŠ‚ç‚¹æ•°={node_count}")

                # ä¸‹è½½æ–‡ä»¶å¹¶æ”¾å…¥é˜Ÿåˆ—
                file_nodes = target_db.get_file_nodes(task_id, pruned_only=True)
                downloaded_count = 0

                for node in file_nodes:
                    try:
                        # åˆ›å»ºæ–‡ä»¶è®°å½•
                        file_id = target_db.create_file_record(
                            task_id=task_id,
                            node_id=node.id,
                            original_url=node.url,
                            original_name=node.title,
                            file_extension=node.file_extension
                        )

                        # ä¸‹è½½æ–‡ä»¶
                        result = downloader.download_file(node.url, task_folder=f"task_{task_id}")

                        if result.success:
                            # ä¸Šä¼ åˆ° Storage
                            remote_path = f"task_{task_id}/raw/{result.file_name}"
                            storage_path = storage.upload_file(result.local_path, remote_path)

                            target_db.update_file_download(
                                file_id, 'completed',
                                storage_path=storage_path,
                                file_size=result.file_size
                            )

                            # æ”¾å…¥æ–‡ä»¶é˜Ÿåˆ—ä¾› Docling å¤„ç†
                            file_task = {
                                'task_id': task_id,
                                'file_id': file_id,
                                'local_path': result.local_path,
                                'original_url': node.url,
                                'original_name': node.title,
                                'context': {
                                    'url': node.url,
                                    'original_name': node.title or '',
                                    'breadcrumb': node.breadcrumb or '',
                                    'title': node.title or '',
                                    'parent_title': '',
                                    'school_name': ''
                                }
                            }
                            file_queue.put(file_task)
                            downloaded_count += 1
                            logger.info(f"[Worker-{worker_id}] ä¸‹è½½æˆåŠŸ: {result.file_name}")
                        else:
                            target_db.update_file_download(file_id, 'failed', error_message=result.error_message)

                    except Exception as e:
                        logger.error(f"[Worker-{worker_id}] ä¸‹è½½å¤±è´¥: {e}")

                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                target_db.update_task_status(task_id, 'processing', node_count=node_count, file_count=downloaded_count)

                # è®°å½•ç»“æœ
                result_dict[link_id] = {
                    'success': True,
                    'task_id': task_id,
                    'node_count': node_count,
                    'file_count': downloaded_count
                }

                logger.info(f"[Worker-{worker_id}] å®Œæˆ task_id={task_id}, æ–‡ä»¶æ•°={downloaded_count}")

            except Empty:
                continue
            except Exception as e:
                logger.error(f"[Worker-{worker_id}] é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                continue

    except Exception as e:
        logger.error(f"Chrome Worker {worker_id} åˆå§‹åŒ–å¤±è´¥: {e}")

    finally:
        if chrome:
            try:
                chrome.quit()
            except:
                pass
        if target_db:
            try:
                target_db.close()
            except:
                pass
        logger.info(f"Chrome Worker {worker_id} é€€å‡º")


# ============================================================
# Docling GPU Worker
# ============================================================

def docling_worker(
    worker_id: int,
    file_queue: Queue,
    text_queue: Queue,
    config: Dict,
    stop_event
):
    """
    Docling GPU Worker

    ä» file_queue è·å–æ–‡ä»¶ï¼Œè§£æåå°†æ–‡æœ¬æ”¾å…¥ text_queue
    """
    logger = logging.getLogger(f"Docling-{worker_id}")
    logger.info(f"Docling Worker {worker_id} å¯åŠ¨")

    # è®¾ç½® GPU
    if config['use_gpu']:
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # æ‰€æœ‰ worker å…±äº« GPU

    # åˆå§‹åŒ– Docling
    from processor.pdf_processor import PDFProcessor
    from processor.doc_processor import DocProcessor

    pdf_processor = PDFProcessor(
        max_pages=config['max_pages'],
        use_docling=True,
        force_ocr=False
    )
    doc_processor = DocProcessor(max_paragraphs=50)

    processed_count = 0

    while not stop_event.is_set():
        try:
            # ä»é˜Ÿåˆ—è·å–æ–‡ä»¶ï¼ˆè¶…æ—¶ 10 ç§’ï¼‰
            file_task = file_queue.get(timeout=10)

            if file_task is None:  # åœæ­¢ä¿¡å·
                logger.info(f"Docling Worker {worker_id} æ”¶åˆ°åœæ­¢ä¿¡å·")
                break

            file_id = file_task['file_id']
            local_path = file_task['local_path']

            logger.info(f"[Docling-{worker_id}] å¼€å§‹è§£æ file_id={file_id}")

            start_time = time.time()

            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å¤„ç†å™¨
            ext = os.path.splitext(local_path)[1].lower()

            if ext == '.pdf':
                result = pdf_processor.extract_text(local_path)
            elif ext in ['.doc', '.docx']:
                result = doc_processor.extract_text(local_path)
            else:
                result = type('obj', (object,), {'success': False, 'text': '', 'error_message': f'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}'})()

            extract_time = time.time() - start_time

            # æ”¾å…¥æ–‡æœ¬é˜Ÿåˆ—
            text_result = {
                'file_id': file_id,
                'task_id': file_task['task_id'],
                'success': result.success,
                'text': result.text if result.success else None,
                'error_message': result.error_message if not result.success else None,
                'context': file_task['context'],
                'local_path': local_path,
                'extract_time': extract_time
            }
            text_queue.put(text_result)

            processed_count += 1
            logger.info(f"[Docling-{worker_id}] è§£æå®Œæˆ file_id={file_id}, è€—æ—¶={extract_time:.1f}s, ç´¯è®¡={processed_count}")

        except Empty:
            continue
        except Exception as e:
            logger.error(f"[Docling-{worker_id}] é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            continue

    logger.info(f"Docling Worker {worker_id} é€€å‡º, å…±å¤„ç† {processed_count} ä¸ªæ–‡ä»¶")


# ============================================================
# LLM é‡å‘½å Worker
# ============================================================

def llm_worker(
    text_queue: Queue,
    config: Dict,
    stop_event
):
    """
    LLM é‡å‘½å Worker

    ä» text_queue è·å–æ–‡æœ¬ï¼Œä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè°ƒç”¨ LLM
    """
    logger = logging.getLogger("LLM-Pool")
    logger.info(f"LLM Worker å¯åŠ¨, çº¿ç¨‹æ•°={config['llm_workers']}")

    import Sdata
    from db.target_db import TargetDatabase
    from processor.llm_renamer import LLMRenamer

    # åˆå§‹åŒ–
    target_db = TargetDatabase()
    target_db.connect()

    renamer = LLMRenamer(api_key=Sdata.Dou_Bao_Key)
    renamer.connect()
    renamer.load_prompt_template()

    pending_tasks = []  # å¾…å¤„ç†çš„ä»»åŠ¡
    processed_count = 0

    def process_single(text_result):
        """å¤„ç†å•ä¸ªæ–‡ä»¶çš„ LLM è°ƒç”¨"""
        file_id = text_result['file_id']

        try:
            if not text_result['success'] or not text_result['text']:
                return {
                    'file_id': file_id,
                    'success': False,
                    'error': text_result.get('error_message', 'æ–‡æœ¬æå–å¤±è´¥')
                }

            # è°ƒç”¨ LLM
            rename_result = renamer.rename_from_text(
                text_result['text'],
                text_result['context'],
                os.path.splitext(text_result['local_path'])[1]
            )

            if rename_result.success and rename_result.renamed_name:
                # æ›´æ–°æ•°æ®åº“
                target_db.update_file_renamed(
                    file_id,
                    renamed_name=rename_result.renamed_name,
                    llm_model=renamer.model,
                    llm_confidence=rename_result.confidence,
                    llm_raw_response=rename_result.raw_response
                )

                return {
                    'file_id': file_id,
                    'success': True,
                    'renamed_name': rename_result.renamed_name
                }
            else:
                return {
                    'file_id': file_id,
                    'success': False,
                    'error': rename_result.error_message
                }

        except Exception as e:
            return {
                'file_id': file_id,
                'success': False,
                'error': str(e)
            }
        finally:
            # æ¸…ç†æœ¬åœ°æ–‡ä»¶
            try:
                if os.path.exists(text_result['local_path']):
                    os.remove(text_result['local_path'])
            except:
                pass

    # ä½¿ç”¨çº¿ç¨‹æ± 
    with ThreadPoolExecutor(max_workers=config['llm_workers']) as executor:
        futures = {}

        while not stop_event.is_set():
            try:
                # å°è¯•è·å–æ–°ä»»åŠ¡
                try:
                    text_result = text_queue.get(timeout=2)

                    if text_result is None:  # åœæ­¢ä¿¡å·
                        logger.info("LLM Worker æ”¶åˆ°åœæ­¢ä¿¡å·")
                        break

                    # æäº¤åˆ°çº¿ç¨‹æ± 
                    future = executor.submit(process_single, text_result)
                    futures[future] = text_result['file_id']

                except Empty:
                    pass

                # æ£€æŸ¥å·²å®Œæˆçš„ä»»åŠ¡
                done_futures = [f for f in futures if f.done()]
                for future in done_futures:
                    file_id = futures.pop(future)
                    try:
                        result = future.result()
                        processed_count += 1
                        if result['success']:
                            logger.info(f"[LLM] é‡å‘½åæˆåŠŸ file_id={file_id}: {result.get('renamed_name', '')}")
                        else:
                            logger.warning(f"[LLM] é‡å‘½åå¤±è´¥ file_id={file_id}: {result.get('error', '')}")
                    except Exception as e:
                        logger.error(f"[LLM] å¤„ç†é”™è¯¯ file_id={file_id}: {e}")

            except Exception as e:
                logger.error(f"[LLM] ä¸»å¾ªç¯é”™è¯¯: {e}")
                continue

        # ç­‰å¾…å‰©ä½™ä»»åŠ¡å®Œæˆ
        logger.info(f"ç­‰å¾…å‰©ä½™ {len(futures)} ä¸ª LLM ä»»åŠ¡å®Œæˆ...")
        for future in as_completed(futures):
            try:
                result = future.result()
                processed_count += 1
            except:
                pass

    target_db.close()
    logger.info(f"LLM Worker é€€å‡º, å…±å¤„ç† {processed_count} ä¸ªæ–‡ä»¶")


# ============================================================
# ä¸» Pipeline
# ============================================================

class Pipeline:
    """ä¸» Pipeline åè°ƒå™¨"""

    def __init__(self, config: PipelineConfig):
        self.config = config
        self.logger = setup_logging(config.log_level)

        # è¿›ç¨‹ç®¡ç†
        self.manager = Manager()
        self.task_queue = Queue()      # å¾…çˆ¬å–çš„é“¾æ¥
        self.file_queue = Queue()      # å¾…è§£æçš„æ–‡ä»¶
        self.text_queue = Queue()      # å¾…é‡å‘½åçš„æ–‡æœ¬
        self.result_dict = self.manager.dict()  # ç»“æœ
        self.stop_event = self.manager.Event()

        self.chrome_processes = []
        self.docling_processes = []
        self.llm_process = None

        # ä¿¡å·å¤„ç†
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _signal_handler(self, signum, frame):
        self.logger.warning("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
        self.stop()

    def start_workers(self):
        """å¯åŠ¨æ‰€æœ‰ Worker"""
        config_dict = {
            'crawl_depth': self.config.crawl_depth,
            'use_gpu': self.config.use_gpu,
            'max_pages': self.config.max_pages,
            'llm_workers': self.config.llm_workers
        }

        # å¯åŠ¨ Chrome Workers
        self.logger.info(f"å¯åŠ¨ {self.config.chrome_workers} ä¸ª Chrome Worker...")
        for i in range(self.config.chrome_workers):
            p = Process(
                target=chrome_worker,
                args=(i, self.task_queue, self.file_queue, self.result_dict, config_dict, self.stop_event),
                name=f"Chrome-{i}"
            )
            p.start()
            self.chrome_processes.append(p)
            time.sleep(2)  # é”™å¼€å¯åŠ¨ï¼Œé¿å…èµ„æºäº‰æŠ¢

        # å¯åŠ¨ Docling GPU Workers
        self.logger.info(f"å¯åŠ¨ {self.config.docling_workers} ä¸ª Docling GPU Worker...")
        for i in range(self.config.docling_workers):
            p = Process(
                target=docling_worker,
                args=(i, self.file_queue, self.text_queue, config_dict, self.stop_event),
                name=f"Docling-{i}"
            )
            p.start()
            self.docling_processes.append(p)
            time.sleep(1)

        # å¯åŠ¨ LLM Worker
        self.logger.info(f"å¯åŠ¨ LLM Worker (çº¿ç¨‹æ•°={self.config.llm_workers})...")
        self.llm_process = Process(
            target=llm_worker,
            args=(self.text_queue, config_dict, self.stop_event),
            name="LLM-Pool"
        )
        self.llm_process.start()

        self.logger.info("æ‰€æœ‰ Worker å·²å¯åŠ¨")

    def stop(self):
        """åœæ­¢æ‰€æœ‰ Worker"""
        self.logger.info("æ­£åœ¨åœæ­¢æ‰€æœ‰ Worker...")
        self.stop_event.set()

        # å‘é€åœæ­¢ä¿¡å·
        for _ in range(self.config.chrome_workers):
            self.task_queue.put(None)
        for _ in range(self.config.docling_workers):
            self.file_queue.put(None)
        self.text_queue.put(None)

        # ç­‰å¾…è¿›ç¨‹ç»“æŸ
        for p in self.chrome_processes:
            p.join(timeout=30)
        for p in self.docling_processes:
            p.join(timeout=30)
        if self.llm_process:
            self.llm_process.join(timeout=30)

        self.logger.info("æ‰€æœ‰ Worker å·²åœæ­¢")

    def get_pending_links(self, limit: int) -> List:
        """è·å–å¾…å¤„ç†çš„é“¾æ¥"""
        from db.source_db import SourceDatabase
        from db.target_db import TargetDatabase
        from sync.incremental_sync import IncrementalSync

        source_db = SourceDatabase()
        source_db.connect()
        target_db = TargetDatabase()
        target_db.connect()

        sync = IncrementalSync(source_db, target_db)
        pending = sync.get_pending_links(include_failed=True, include_changed=True)

        if self.config.link_type:
            pending = [l for l in pending if l.table_name == self.config.link_type]

        pending = pending[:limit]
        result = [(l.id, l.url, l.table_name) for l in pending]

        source_db.close()
        target_db.close()

        return result

    def run(self):
        """è¿è¡Œ Pipelineï¼ˆæµæ°´çº¿æ¨¡å¼ï¼ŒChrome å’Œ Docling äº¤å‰è¿è¡Œï¼‰"""
        self.logger.info("=" * 60)
        self.logger.info("Pipeline å¯åŠ¨ (æµæ°´çº¿æ¨¡å¼)")
        self.logger.info(f"é…ç½®:")
        self.logger.info(f"  Chrome Workers:  {self.config.chrome_workers}")
        self.logger.info(f"  Docling Workers: {self.config.docling_workers}")
        self.logger.info(f"  LLM Workers:     {self.config.llm_workers}")
        self.logger.info(f"  Batch Size:      {self.config.batch_size}")
        self.logger.info(f"  Crawl Depth:     {self.config.crawl_depth}")
        self.logger.info(f"  Use GPU:         {self.config.use_gpu}")
        self.logger.info("=" * 60)

        # å¯åŠ¨ Workers
        self.start_workers()

        batch_count = 0
        total_queued = 0
        start_time = time.time()

        try:
            while not self.stop_event.is_set():
                # æ£€æŸ¥æ‰¹æ¬¡é™åˆ¶
                if self.config.max_batches > 0 and batch_count >= self.config.max_batches:
                    self.logger.info(f"å·²è¾¾åˆ°æœ€å¤§æ‰¹æ¬¡æ•° {self.config.max_batches}")
                    break

                # æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦éœ€è¦è¡¥å……ä»»åŠ¡
                # ä¿æŒé˜Ÿåˆ—ä¸­æœ‰è¶³å¤Ÿçš„ä»»åŠ¡ï¼Œè®© workers æŒç»­å·¥ä½œ
                queue_size = self.task_queue.qsize()

                # å½“é˜Ÿåˆ—ä¸­ä»»åŠ¡å°‘äº chrome_workers * 2 æ—¶ï¼Œè¡¥å……æ–°ä»»åŠ¡
                if queue_size < self.config.chrome_workers * 2:
                    # è·å–å¾…å¤„ç†é“¾æ¥
                    pending = self.get_pending_links(self.config.batch_size)

                    if not pending:
                        # æ²¡æœ‰æ–°ä»»åŠ¡äº†ï¼Œç­‰å¾…ç°æœ‰ä»»åŠ¡å®Œæˆ
                        if queue_size == 0 and len(self.result_dict) >= total_queued:
                            self.logger.info("æ²¡æœ‰æ›´å¤šå¾…å¤„ç†ä»»åŠ¡")
                            break
                        else:
                            # è¿˜æœ‰ä»»åŠ¡åœ¨å¤„ç†ä¸­ï¼Œç­‰å¾…
                            time.sleep(5)
                            continue

                    batch_count += 1
                    self.logger.info(f"\n===== è¡¥å……æ‰¹æ¬¡ {batch_count} ({len(pending)} ä¸ªä»»åŠ¡) =====")

                    # å°†ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰
                    for link_data in pending:
                        self.task_queue.put(link_data)

                    total_queued += len(pending)
                    self.logger.info(f"é˜Ÿåˆ—çŠ¶æ€: å·²æŠ•æ”¾ {total_queued}, å·²å®Œæˆ {len(self.result_dict)}")

                # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯ 10 ç§’ï¼‰
                time.sleep(10)

                completed = len(self.result_dict)
                elapsed = (time.time() - start_time) / 60
                rate = completed / elapsed if elapsed > 0 else 0

                # è·å–é˜Ÿåˆ—çŠ¶æ€
                task_q = self.task_queue.qsize()
                file_q = self.file_queue.qsize()
                text_q = self.text_queue.qsize()

                self.logger.info(
                    f"[è¿›åº¦] å®Œæˆ: {completed}/{total_queued} | "
                    f"é˜Ÿåˆ—: çˆ¬å–={task_q}, è§£æ={file_q}, é‡å‘½å={text_q} | "
                    f"é€Ÿç‡: {rate:.1f}/åˆ†é’Ÿ"
                )

        except KeyboardInterrupt:
            self.logger.info("ç”¨æˆ·ä¸­æ–­")

        finally:
            # ç­‰å¾…é˜Ÿåˆ—å¤„ç†å®Œ
            self.logger.info("ç­‰å¾…å‰©ä½™ä»»åŠ¡å®Œæˆ...")

            # ç­‰å¾…æ‰€æœ‰é˜Ÿåˆ—æ¸…ç©º
            wait_count = 0
            while wait_count < 30:  # æœ€å¤šç­‰å¾… 5 åˆ†é’Ÿ
                task_q = self.task_queue.qsize()
                file_q = self.file_queue.qsize()
                text_q = self.text_queue.qsize()

                if task_q == 0 and file_q == 0 and text_q == 0:
                    self.logger.info("æ‰€æœ‰é˜Ÿåˆ—å·²æ¸…ç©º")
                    break

                self.logger.info(f"ç­‰å¾…é˜Ÿåˆ—æ¸…ç©º: çˆ¬å–={task_q}, è§£æ={file_q}, é‡å‘½å={text_q}")
                time.sleep(10)
                wait_count += 1

            self.stop()

            # æœ€ç»ˆç»Ÿè®¡
            elapsed = (time.time() - start_time) / 60
            total_processed = len(self.result_dict)
            self.logger.info("\n" + "=" * 60)
            self.logger.info("Pipeline ç»“æŸ")
            self.logger.info(f"æ€»æ‰¹æ¬¡: {batch_count}")
            self.logger.info(f"æ€»å¤„ç†: {total_processed}")
            self.logger.info(f"æ€»è€—æ—¶: {elapsed:.1f} åˆ†é’Ÿ")
            if elapsed > 0:
                self.logger.info(f"å¹³å‡é€Ÿç‡: {total_processed/elapsed:.1f} ä¸ª/åˆ†é’Ÿ")
            self.logger.info("=" * 60)


# ============================================================
# å‘½ä»¤è¡Œå…¥å£
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description='é«˜æ€§èƒ½å¹¶è¡Œå¤„ç† Pipeline',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è‡ªåŠ¨æ£€æµ‹èµ„æºå¹¶é…ç½®ï¼ˆæ¨èï¼‰
  python run_pipeline.py --auto

  # åªæŸ¥çœ‹èµ„æºå’Œæ¨èé…ç½®
  python run_pipeline.py --resources

  # æ‰‹åŠ¨é…ç½®
  python run_pipeline.py --chrome 4 --docling 3 --llm 25 --batch 20

  # é«˜æ€§èƒ½é…ç½® (å¤§å†…å­˜æœåŠ¡å™¨)
  python run_pipeline.py --chrome 6 --docling 4 --llm 30 --batch 30

  # ä½èµ„æºé…ç½®
  python run_pipeline.py --chrome 2 --docling 2 --llm 10 --batch 10
        """
    )

    # è‡ªåŠ¨æ¨¡å¼
    parser.add_argument('--auto', '-a', action='store_true',
                        help='è‡ªåŠ¨æ£€æµ‹èµ„æºå¹¶é…ç½®æœ€ä¼˜å‚æ•°ï¼ˆæ¨èï¼‰')
    parser.add_argument('--resources', action='store_true',
                        help='åªæ˜¾ç¤ºç³»ç»Ÿèµ„æºå’Œæ¨èé…ç½®ï¼Œä¸è¿è¡Œ')

    # Chrome é…ç½®
    parser.add_argument('--chrome', '-c', type=int, default=None,
                        help='Chrome çˆ¬è™«è¿›ç¨‹æ•° (é»˜è®¤: è‡ªåŠ¨)')
    parser.add_argument('--depth', '-d', type=int, default=1,
                        help='çˆ¬å–æ·±åº¦ (é»˜è®¤: 1)')

    # Docling é…ç½®
    parser.add_argument('--docling', '-g', type=int, default=None,
                        help='Docling GPU è§£æè¿›ç¨‹æ•° (é»˜è®¤: è‡ªåŠ¨)')
    parser.add_argument('--no-gpu', action='store_true',
                        help='ç¦ç”¨ GPUï¼Œä½¿ç”¨ CPU è§£æ')
    parser.add_argument('--max-pages', type=int, default=2,
                        help='PDF æœ€å¤§æå–é¡µæ•° (é»˜è®¤: 2)')

    # LLM é…ç½®
    parser.add_argument('--llm', '-l', type=int, default=None,
                        help='LLM å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: è‡ªåŠ¨)')

    # ä»»åŠ¡é…ç½®
    parser.add_argument('--batch', '-b', type=int, default=None,
                        help='æ¯æ‰¹å¤„ç†ä»»åŠ¡æ•° (é»˜è®¤: è‡ªåŠ¨)')
    parser.add_argument('--type', '-t', choices=['graduate', 'undergraduate'],
                        help='åªå¤„ç†æŒ‡å®šç±»å‹')
    parser.add_argument('--max-batches', '-m', type=int, default=0,
                        help='æœ€å¤§æ‰¹æ¬¡æ•° (0=æ— é™åˆ¶)')
    parser.add_argument('--rest', '-r', type=int, default=30,
                        help='æ‰¹æ¬¡é—´ä¼‘æ¯æ—¶é—´(ç§’) (é»˜è®¤: 30)')

    # å…¶ä»–
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                        default='INFO', help='æ—¥å¿—çº§åˆ«')
    parser.add_argument('--status', '-s', action='store_true',
                        help='åªæ˜¾ç¤ºå½“å‰è¿›åº¦')

    args = parser.parse_args()

    # åªæ˜¾ç¤ºèµ„æºä¿¡æ¯
    if args.resources:
        config, resources = auto_detect_config()
        print("\nä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œ:")
        print(f"  python run_pipeline.py --chrome {config.chrome_workers} --docling {config.docling_workers} --llm {config.llm_workers} --batch {config.batch_size}")
        print("\næˆ–ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼:")
        print("  python run_pipeline.py --auto")
        return

    # æ˜¾ç¤ºçŠ¶æ€
    if args.status:
        from db.source_db import SourceDatabase
        from db.target_db import TargetDatabase

        source_db = SourceDatabase()
        source_db.connect()
        target_db = TargetDatabase()
        target_db.connect()

        graduate = source_db.get_count_by_type('graduate')
        undergraduate = source_db.get_count_by_type('undergraduate')
        total = graduate + undergraduate
        completed = len(target_db.get_tasks_by_status('completed'))
        failed = len(target_db.get_tasks_by_status('failed'))
        processing = len(target_db.get_tasks_by_status('processing'))

        print(f"""
========== å¤„ç†è¿›åº¦ ==========
æ€»ä»»åŠ¡æ•°:   {total}
å·²å®Œæˆ:     {completed}
å¤„ç†ä¸­:     {processing}
å·²å¤±è´¥:     {failed}
å‰©ä½™:       {total - completed}
å®Œæˆç‡:     {completed/total*100:.1f}%
==============================
        """)

        source_db.close()
        target_db.close()
        return

    # è‡ªåŠ¨æ¨¡å¼æˆ–ä»»ä½•å‚æ•°æœªæŒ‡å®šæ—¶ï¼Œä½¿ç”¨è‡ªåŠ¨æ£€æµ‹
    if args.auto or (args.chrome is None and args.docling is None and args.llm is None):
        print("\nğŸ” è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿèµ„æº...")
        auto_config, resources = auto_detect_config()

        # ä½¿ç”¨è‡ªåŠ¨é…ç½®ï¼Œä½†å…è®¸å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
        config = PipelineConfig(
            chrome_workers=args.chrome if args.chrome is not None else auto_config.chrome_workers,
            crawl_depth=args.depth,
            docling_workers=args.docling if args.docling is not None else auto_config.docling_workers,
            use_gpu=not args.no_gpu and auto_config.use_gpu,
            max_pages=args.max_pages,
            llm_workers=args.llm if args.llm is not None else auto_config.llm_workers,
            batch_size=args.batch if args.batch is not None else auto_config.batch_size,
            link_type=args.type,
            max_batches=args.max_batches,
            rest_time=args.rest,
            log_level=args.log_level
        )
    else:
        # æ‰‹åŠ¨æ¨¡å¼ï¼Œä½¿ç”¨æŒ‡å®šçš„å‚æ•°ï¼ˆé»˜è®¤å€¼ä½œä¸ºåå¤‡ï¼‰
        config = PipelineConfig(
            chrome_workers=args.chrome if args.chrome is not None else 4,
            crawl_depth=args.depth,
            docling_workers=args.docling if args.docling is not None else 3,
            use_gpu=not args.no_gpu,
            max_pages=args.max_pages,
            llm_workers=args.llm if args.llm is not None else 25,
            batch_size=args.batch if args.batch is not None else 20,
            link_type=args.type,
            max_batches=args.max_batches,
            rest_time=args.rest,
            log_level=args.log_level
        )

    # æ˜¾ç¤ºæœ€ç»ˆé…ç½®
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              Pipeline æœ€ç»ˆé…ç½®                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Chrome çˆ¬è™«è¿›ç¨‹:    {config.chrome_workers:3d}                                  â•‘
â•‘  Docling GPU è¿›ç¨‹:   {config.docling_workers:3d}                                  â•‘
â•‘  LLM å¹¶å‘çº¿ç¨‹:       {config.llm_workers:3d}                                  â•‘
â•‘  æ¯æ‰¹ä»»åŠ¡æ•°:         {config.batch_size:3d}                                  â•‘
â•‘  çˆ¬å–æ·±åº¦:           {config.crawl_depth:3d}                                  â•‘
â•‘  ä½¿ç”¨ GPU:           {'æ˜¯' if config.use_gpu else 'å¦':3s}                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # è¿è¡Œ
    pipeline = Pipeline(config)
    pipeline.run()


if __name__ == "__main__":
    main()
