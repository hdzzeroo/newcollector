
#from Support import *
#ç±»è®¾è®¡çš„æ ¸å¿ƒæ€è·¯

from selenium import webdriver                              # seleniumçš„å¯¹è±¡
from selenium.webdriver.chrome.options import Options       # seleniumçš„æ ¸å¿ƒ
from selenium.webdriver.chrome.service import Service       # è§£å†³æ— é™åŠ è½½é—®é¢˜
from webdriver_manager.chrome import ChromeDriverManager    # è§£å†³æ— é™åŠ è½½é—®é¢˜
from selenium.webdriver.support.ui import WebDriverWait     # æ£€æµ‹è¶…æ—¶é—®é¢˜
from selenium.common.exceptions import TimeoutException     # è¶…æ—¶æŠ¥é”™
from selenium.common.exceptions import WebDriverException   # æµè§ˆå™¨é©±åŠ¨æŠ¥é”™
from selenium.webdriver.common.by import By                 # äººé™…éªŒè¯ç”¨

from collections import namedtuple              # è½»é‡åŒ–ç»“æ„ä½“
from os.path import exists as OPAexists         # æ£€æŸ¥æŸæ–‡ä»¶æ˜¯å¦å­˜åœ¨
from os.path import isdir as OSisdir            # æ£€æŸ¥æŸè·¯å¾„æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
from os import system as Osm, times             # æ¸…ç©ºæ§åˆ¶å°
from os import makedirs   as OSmakedirs         # åˆ›å»ºæ–‡ä»¶å¤¹
from sys import stdout                          # DEBUGåº“ç”¨çš„
from sys import exit as sysExit                 # é€€å‡ºç¨‹åº
from time import sleep as Tsleep                # ä¼‘çœ ä¸€ä¼š

from json_repair import loads                               #jsonä¿®å¤
from collections import defaultdict                         # çˆ¶å­èŠ‚ç‚¹
from urllib.parse import urljoin, urlparse, urlunparse      # å¤„ç†url
from bs4 import BeautifulSoup                               # è§£æé¡µé¢ä¿¡æ¯ç”¨
import markdownify                                          # MDæ–‡ä»¶ç”¨

import math                                                         #æ•°å­¦è¿ç®—
import struct                                                       #ç»“æ„
import io                                                           #è¾“å…¥è¾“å‡º
import wave                                                         #æ³¢å½¢
import platform                                                     #å¹³å°æ£€æµ‹
import csv
import json
from openai import OpenAI
import re
import time

import Sdata
__all__ = ['OverView', 'OverView_FloorMode','DebugPrinter']

#
HEURISTIC_KEYWORDS	= Sdata.HEURISTIC_KEYWORDS
CORE_KEYWORDS		= Sdata.CORE_KEYWORDS
BLACKLIST			= Sdata.BLACKLIST

DATA_BASE			= Sdata.DATA_BASE
OUTPUT_MD			= Sdata.OUTPUT_MD    
OUTPUT_FOLDER       = Sdata.OUTPUT_FOLDER
CSV_FILENAME        = Sdata.CSV_FILENAME
HTML_FILENAME       = Sdata.HTML_FILENAME
HTMLED_FILENAME       = Sdata.HTMLED_FILENAME

FILE_EXTENSIONS     = Sdata.FILE_EXTENSIONS
CSVCLEANED_FILENAME = Sdata.CSVCLEANED_FILENAME

BasicDepth    =  -1
PACK_MAX_SIZE       = Sdata.PACK_MAX_SIZE
blacklist_regex = "|".join(map(re.escape, BLACKLIST))           #æ­£åˆ™è¡¨è¾¾å¼çš„
heuristic_regex = "|".join(map(re.escape, HEURISTIC_KEYWORDS))  #æ­£åˆ™è¡¨è¾¾å¼çš„

EXPORT_PDF      = "pdfCollect.csv"

#å®šä¹‰èŠ‚ç‚¹ç»“æ„ä½“
NodeStruct = namedtuple("NodeStruct", ["Index", "url", "Depth","FatherIndex"])  

#è±†åŒ…
Doubao_32 = OpenAI(
    # æ­¤ä¸ºé»˜è®¤è·¯å¾„ï¼Œæ‚¨å¯æ ¹æ®ä¸šåŠ¡æ‰€åœ¨åœ°åŸŸè¿›è¡Œé…ç½®
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    # ä»ç¯å¢ƒå˜é‡ä¸­è·å–æ‚¨çš„ API Key
    api_key= Sdata.Dou_Bao_Key,
)
#è±†åŒ…2
Doubao_256 = OpenAI(
    # æ­¤ä¸ºé»˜è®¤è·¯å¾„ï¼Œæ‚¨å¯æ ¹æ®ä¸šåŠ¡æ‰€åœ¨åœ°åŸŸè¿›è¡Œé…ç½®
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    # ä»ç¯å¢ƒå˜é‡ä¸­è·å–æ‚¨çš„ API Key
    api_key= Sdata.Dou_Bao_Key,
)
#è·å¾—chromeçš„å¯¹è±¡å¹¶åˆå§‹åŒ–
def overViewInit():
    create_Folder(OUTPUT_FOLDER)
    create_Folder(DATA_BASE)

    #1.åˆå§‹åŒ–Chrome
    chrome_options = Options()
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--log-level=3')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--headless=new')  # æ–°ç‰ˆ headless æ¨¡å¼æ›´ç¨³å®š
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--disable-software-rasterizer')
    chrome_options.add_argument('--disable-extensions')
    chrome_options.add_argument('--disable-background-networking')
    chrome_options.add_argument('--disable-sync')
    chrome_options.add_argument('--disable-translate')
    chrome_options.add_argument('--no-first-run')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.page_load_strategy = 'eager'
    chrome_options.add_argument('--blink-settings=imagesEnabled=false')
    # å¼ºåˆ¶æµè§ˆå™¨ä¸å¼¹å‡ºä¸‹è½½çª—å£ï¼Œä¸”ç¦æ­¢ PDF è‡ªåŠ¨ä¸‹è½½
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "download.default_directory": "/dev/null",
        "download.prompt_for_download": False,
        "plugins.always_open_pdf_externally": False,
    }
    chrome_options.add_experimental_option("prefs", prefs)

    # å¼ºåˆ¶ 20ç§’ è¶…æ—¶     ---     è§£å†³æŸäº›é¡µé¢æ— é™åŠ è½½/åˆ·æ–°çš„é—®é¢˜
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    driver.set_page_load_timeout(25) 
    return driver



class OverView():
    def __init__(self,targetUrl:str,depth:int,sign:str = "å¤§å­¦æ•°æ®"):
        #1.åŸºç¡€å‚æ•°
        
        self.starturl    = targetUrl
        self.oriUrl      = targetUrl
        self._MAX_DEPTH  = depth        #æ£€ç´¢æ·±åº¦ï¼ˆé‡è¦ï¼‰
        self.isStart     = False
        self.chrome      = None
        
        
        self.BaseSign    = sign
        #2.å¤æ‚å‚æ•°
        self.Uqueue      =  []      #é˜Ÿåˆ—
        self.URL_LAB     =  {}      #åªç”¨æ¥å­˜ç°æœ‰çš„urlç§ç±» ï¼Œæ¯ä¸ªurlæ˜ å°„ä¸€ä¸ªINDEX
        self.URL_RLAB    =  {}      #æ¯ä¸ªIndexæ˜ å°„ä¸€ä¸ªurl
        self.visitedUrls   = set() #æµè§ˆè¿‡çš„ä»»åŠ¡
        
        self._MAX_DEPTH     = self._MAX_DEPTH if self._MAX_DEPTH < 11 else 10
        self.MemPath     = OUTPUT_FOLDER + "/" + self.BaseSign
    def SetOriUrl(self,url):
        self.oriUrl      =  url
    # [åŠŸèƒ½]æ’å…¥é˜Ÿåˆ—èŠ‚ç‚¹
    def AddNode(self,url:str,fatherIndex:int,fatherDepth:int,title:str,Breadcrumb:str,message:str):
        #æ·»åŠ è¿›åŒå‘æ˜ å°„è¡¨BreadcrumbList
        if url not in self.URL_LAB:
            _nowSize = len(self.URL_LAB.keys())
            self.URL_LAB[url] = _nowSize
            self.URL_RLAB[str(_nowSize)] = [url,_nowSize,fatherIndex,fatherDepth+1,title,Breadcrumb,message]   
            #ç°åœ¨é“¾æ¥ è‡ªå·±çš„Index çˆ¶äº²Index  æ·±åº¦ ,é¢åŒ…å±‘è·¯å¾„ 100å­—æ‘˜è¦
       
        #æ·»åŠ è¿™ä¸ªèŠ‚ç‚¹è¿›å…¥é˜Ÿåˆ—é‡Œ
        self.Uqueue.append(NodeStruct(int(self.URL_LAB[url]),url,fatherDepth+1,fatherIndex))
        
    # [åŠŸèƒ½]åˆå§‹åŒ–,å¯åŠ¨å‰è¯·è¿è¡Œä»–
    def start(self,chrome:webdriver.Chrome):
        # åˆå§‹åŒ–åˆ—è¡¨ä¿¡æ¯
        self.AddNode(self.starturl,BasicDepth,BasicDepth,"æ ¹èŠ‚ç‚¹","å¼€å§‹ç‚¹","æ— ")
        create_Folder(self.MemPath)
        # å¯åŠ¨æµè§ˆå™¨
        self.chrome = chrome
        self.is_start = True
        try:
            chrome.get(self.starturl )
        except:
            print(f"å¯åŠ¨å¼‚å¸¸")
            
    # [åŠŸèƒ½]è¿è¡Œç»“æŸåæ‰‹åŠ¨å…³é—­chromeï¼Œé‡Šæ”¾å†…å®¹
    def end(self):
        
        self.URL_LAB.clear()
        self.URL_RLAB.clear()
        self.URL_LAB  = []
        self.URL_RLAB = []
        self.visitedUrls.clear()
        self.visitedUrls   = set() 
        if self.chrome:
            try:
                # 1. æ¸…é™¤å½“å‰ç«™ç‚¹çš„ Cookies å’Œ LocalStorage (é˜²æ­¢çŠ¶æ€æ±¡æŸ“)
                self.chrome.delete_all_cookies()
                
                # 2. å¯¼èˆªåˆ°ç©ºç™½é¡µ (about:blank) 
                # è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ï¼šè¿™ä¼šå¼ºåˆ¶æµè§ˆå™¨å¸è½½å½“å‰é¡µé¢çš„ DOMã€JS å¼•æ“å’Œæ’ä»¶å†…å­˜
                self.chrome.get("about:blank")
                # 3. å¦‚æœä½ æ‰“å¼€äº†å¤šä¸ªçª—å£ï¼Œåªä¿ç•™å½“å‰è¿™ä¸€ä¸ªï¼Œå…³é—­å…¶ä»–çš„
                handles = self.chrome.window_handles
                while len(handles) > 1:
                    self.chrome.switch_to.window(handles[-1])
                    self.chrome.close()
                    handles = self.chrome.window_handles
                self.chrome.switch_to.window(handles[0])

            except Exception as e:
                print(f"[-] æ¸…ç†è¿‡ç¨‹å¼‚å¸¸: {e}")
        
        # è¿”å›åˆå§‹ URLï¼Œæ–¹ä¾¿ä¸‹æ¬¡ç›´æ¥å†æ¬¡è°ƒç”¨
        return self.starturl
    
    # [åŠŸèƒ½]å¹¿åº¦ä¼˜å…ˆæ–¹æ¡ˆ
    def Seek(self):
        TPA = time.time()
        OKNoise()

        while self.Uqueue:
            Node = self.Uqueue.pop(0)
            seeIndex = Node.Index           #æ­£åœ¨æµè§ˆçš„ç½‘å€çš„Index - ç”¨äºç”Ÿæˆå­é›†èŠ‚ç‚¹çš„çˆ¶Index
            _Url     = Node.url             #å½“å‰è¦æµè§ˆçš„url
            DepthNow =Node.Depth            #æ­£åœ¨æµè§ˆçš„æ·±åº¦        - ç”¨äºç»™å­èŠ‚ç‚¹å®šä¹‰æ·±åº¦
            #Node.FatherIndex               #åº”è¯¥æš‚æ—¶æ²¡ä»€ä¹ˆç”¨
            
            #å‚è§‚è¿‡çš„ä¸åŠ 
            if _Url in self.visitedUrls:
                continue

            #è™½ç„¶æ·±åº¦å¤§ä¸å¤„ç†ä½†æ˜¯ä¹ŸåŠ 
            if DepthNow > self._MAX_DEPTH  or (any(_Url.lower().endswith(ext) for ext in FILE_EXTENSIONS)):
                print(f"-å®Œæˆ: {len(self.visitedUrls)} | -å‰©ä½™ï¼š {len(self.Uqueue)+1} | -æ·±åº¦: {DepthNow}")
                self.visitedUrls.add(_Url) 
                # 1. åˆ¤æ–­åç¼€æ˜¯å¦ä¸º .pdf
                if _Url.lower().endswith('.pdf'):

                    # 2. ä»¥ a+ æ¨¡å¼æ‰“å¼€ CSV æ–‡ä»¶
                    # newline='' æ˜¯ä¸ºäº†é˜²æ­¢åœ¨æŸäº›æ“ä½œç³»ç»Ÿï¼ˆå¦‚ Windowsï¼‰ä¸­å‡ºç°å¤šä½™çš„ç©ºè¡Œ
                    with open(EXPORT_PDF, mode='a+', encoding='utf-8', newline='') as f:
                        writer = csv.writer(f)
        
                        # 3. å†™å…¥ä¸€è¡Œæ•°æ®
                        # æ³¨æ„ï¼šwriterow æ¥æ”¶ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ å¯¹åº” CSV çš„ä¸€åˆ—
                        writer.writerow([_Url])
        
                    print(f"å·²å°† '{_Url}' å†™å…¥ {EXPORT_PDF}")
                else:
                    pass
                continue
            try:
                self.chrome.get(_Url)
                # ç­‰å¾… JS æ¸²æŸ“çš„è¡¥å……ï¼šç”¨æ˜¾å¼ç­‰å¾…ä»£æ›¿ç›²ç›®çš„ time.sleep
                try:
                    WebDriverWait(self.chrome, 20).until(
                        lambda d: d.execute_script('return document.readyState') == 'complete'
                    )
                except TimeoutException:
                    # å¦‚æœ 8 ç§’å†…æ²¡åŠ è½½å®Œï¼Œå¼ºåˆ¶åœæ­¢åŠ è½½ï¼Œç›´æ¥è§£æç°æœ‰ DOM
                    self.chrome.execute_script("window.stop();")
                    print(f"æ¸²æŸ“è¶…æ—¶(20s)ï¼Œå·²æˆªæ–­å¹¶è§£æéƒ¨åˆ†å†…å®¹: {_Url}")

                # rå¦‚æœæœ‰äººæœºéªŒè¯ - è·³è¿‡è¯¥ä»»åŠ¡
                if not CookieGo(self.chrome):
                    self.visitedUrls.add(_Url)
                    continue 

                # è§£æé¡µé¢
                soup = BeautifulSoup(self.chrome.page_source, 'html.parser')
                self.visitedUrls.add(_Url) # åªæœ‰æˆåŠŸè·å–åˆ°å†…å®¹æ‰æ ‡è®°ä¸ºå·²è®¿é—®

            except TimeoutException:
                print(f"è®¿é—®è¶…æ—¶ï¼Œè·³è¿‡è¯¥é¡µ: {_Url}")
                try:
                    self.chrome.execute_script("window.stop();") # å°è¯•å¼ºåˆ¶åœæ­¢é¡µé¢åŠ è½½
                except:
                    pass
                self.visitedUrls.add(_Url) # æ ‡è®°ä¸ºå·²è®¿é—®ï¼Œé˜²æ­¢åœ¨é˜Ÿåˆ—é‡Œåå¤å°è¯•
                continue
            except WebDriverException as e:
                print(f"é©±åŠ¨å¼‚å¸¸: {e}")
                self.visitedUrls.add(_Url)
                continue
            except Exception as e:
                print(f"âŒæœªçŸ¥é”™è¯¯: {e}")
                self.visitedUrls.add(_Url)
                continue    
    

            """è¿™ä¸€æ­¥æ˜¯ä¿å­˜å¯¹åº”çš„mdæ–‡ä»¶"""
            if True:
                # è§£æ
                soup = BeautifulSoup(self.chrome.page_source, 'html.parser')
                for tag in soup(["nav", "footer", "header", "script", "style", "aside"]):
                    tag.decompose()
                md_text = markdownify.markdownify(str(soup), heading_style="ATX")   # è½¬ Markdown
                page_description = GetIntroduce(soup, md_text)                      # è·å–æ‘˜è¦
                BreadCrumbs = GetBreadcrumbs(soup)                                  # BRè·¯å¾„æºï¼ï¼
                print(BreadCrumbs)
                seeIndex
                # åˆ¤æ–­æ˜¯å¦ä¸º HOT NODEï¼Œå¹¶æ‰¾å‡ºå‘½ä¸­çš„å…³é”®è¯
                matched_keywords = [word for word in CORE_KEYWORDS if word in md_text]
                is_hot_node = len(matched_keywords) > 0

                if is_hot_node:
                    print(f"ã€+ã€‘å‘½ä¸­å…³é”®è¯: {matched_keywords}")
                else:
                    print("æ²¡æœ‰å…³é”®è¯ï¼Œäºæ˜¯è½¬èº«å‘å±±é‡Œèµ°å»")
                
                path = self.MemPath + "/" + f"INDEX_{seeIndex}.txt"
                
                with open(path, "w", encoding="utf-8") as f:
                    header =   (f"--- \n"
                                f"INDEX: {seeIndex}\n"
                                #f"PARENT_INDEX: {parent_idx}\n"
                                f"URL: {_Url}\n"
                                f"SUMMARY: {md_text}\n"
                                f"--- \n\n")
                    f.write(header + md_text)
    
            """è¿™ä¸€æ­¥æ˜¯å»æ‚ç„¶ååŠ å…¥å¯¹åº”çš„é˜Ÿåˆ—èŠ‚ç‚¹"""
            if True:
                links = soup.find_all('a', href=True)
                for b in links:
                    link_text = b.get_text(strip=True) or "Image/None"      #æ ‡é¢˜
                    _raw_href = b['href']
                    full_url = clean_url(urljoin(_Url, _raw_href))          #é“¾æ¥
            
                    #DEBPrint(link_text,full_url)
        
                    # åŸŸåæ£€æŸ¥
                    if urlparse(self.starturl).netloc not in urlparse(full_url).netloc:
                        continue

                    # é»‘åå•
                    # re.escape ä¼šè‡ªåŠ¨è½¬ä¹‰åˆ—è¡¨é‡Œçš„ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚ google.com é‡Œçš„ç‚¹ï¼‰
                    #blacklist_regex = "|".join(map(re.escape, BLACKLIST))

                    if re.search(blacklist_regex, full_url, re.IGNORECASE) or \
                       re.search(blacklist_regex, (link_text or ""), re.IGNORECASE):
                        continue

                    # åç¼€ç‰¹æ®ŠåŒ–å¤„ç† - åŠ å…¥é˜Ÿåˆ—ä½†æ˜¯æ·±åº¦è®¾ç½®ä¸ºæœ€å¤§
                    if any(full_url.lower().endswith(ext) for ext in FILE_EXTENSIONS):
                        _bs = "" if BreadCrumbs == None else BreadCrumbs + ">PDF"
                        self.AddNode(full_url,seeIndex,DepthNow,link_text,_bs,"PDF")
                        continue
                    #
                    #heuristic_regex = "|".join(map(re.escape, HEURISTIC_KEYWORDS))

                    # ä¿®æ”¹åçš„åˆ¤æ–­é€»è¾‘ - å¦‚æœæ˜¯çƒ­æƒ…æ¨¡å¼ æˆ– åœ°å€é‡Œæœ‰å¥½å‡¶è¥¿ æˆ– æ ‡é¢˜é‡Œæœ‰æ²¡æœ‰å¥½ä¸œè¥¿
                    should_follow = (
                        is_hot_node or 
                        re.search(heuristic_regex, full_url, re.IGNORECASE) or 
                        re.search(heuristic_regex, (link_text or ""), re.IGNORECASE)
                    )
                    #å¦‚æœç¡®å®æ˜¯æˆ‘ä»¬åƒå“Ÿçš„ä¸œè¥¿çš„åŒ–
                    if should_follow:
                        self.AddNode(full_url,seeIndex,DepthNow,link_text,BreadCrumbs,page_description)

            print(f"-å®Œæˆ: {len(self.visitedUrls)} | -å‰©ä½™ï¼š {len(self.Uqueue)+1} | -æ·±åº¦: {DepthNow}")
        
        
        CHECK_Noise()
        _write = []
        URL_RLAB = self.URL_RLAB
        #print(self.URL_LAB)
        for _key in self.URL_RLAB.keys():
            _path     = self.MemPath + "/" + f"INDEX_{_key}.txt"
            #mdMessage = "None" if OPAexists(_path) else "None"
            #[url,_nowSize,fatherIndex,fatherDepth+1,title]
            _write.append({"Index":URL_RLAB[_key][1],
                           "FatherIndex":URL_RLAB[_key][2],
                           "Depth":URL_RLAB[_key][3],
                           "title":URL_RLAB[_key][4],
                           "Breadcrumb":URL_RLAB[_key][5],
                           "Url":URL_RLAB[_key][0],       #GetShortURL(baseUrl, URL_RLAB[_key][0])
                           "FatherTitle":URL_RLAB[ str(URL_RLAB[_key][2]) ][4] if not URL_RLAB[_key][2] == -1 else "è¿™ä¸ªèŠ‚ç‚¹æ²¡æœ‰çˆ¶èŠ‚ç‚¹" ,
                           })

        _pathCsv = self.MemPath + "/" + CSV_FILENAME
        with open(_pathCsv, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ["Index", "FatherIndex", "Depth","title","Breadcrumb","Url", "FatherTitle"]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(_write)
            
  
        
        _pathMd  = self.MemPath + "/" + OUTPUT_MD
        _pathHtml= self.MemPath + "/" + HTML_FILENAME
        _pathCsv = self.MemPath + "/" + CSV_FILENAME
        #ç”ŸæˆGeneMD
        GeneMD(_pathCsv,_pathMd,self.starturl)
        
        #ç”Ÿæˆæ€»HTML
        GeneHtml(_pathCsv,_pathHtml)
  
        DEBPrint("è€—æ—¶ï¼ˆmï¼‰",round((time.time()- TPA)/60,2))
        
    # [åŠŸèƒ½]è£åˆ‡èŠ‚ç‚¹æ“ä½œ
    def Pruning(self):
        _pathMd  = self.MemPath + "/" + OUTPUT_MD
        _pathHtml= self.MemPath + "/" + HTML_FILENAME
        _pathCsv = self.MemPath + "/" + CSV_FILENAME
                #å¤„ç†ç®€åŒ–æ•°æ®ä¾›AIæŸ¥è¯¢
        Easied(_pathCsv,'simpled.csv')
        GeneMD('simpled.csv','SMD.txt',self.starturl,True)
        
        #chunks = packChunks(_pathCsv,self.oriUrl)
        

        #ä¾›AIè¯»å–
        # è¯»å–å‰ªææç¤ºè¯
        with open(DATA_BASE + '/' + "Cutf.txt",mode = 'r',encoding='utf-8') as f:
            prtm = f.read()
        # è¯»å–å¤„ç†å¥½çš„å‰ªæé¢„é‡‡æ ·
        with open("SMD.txt",mode = 'r',encoding='utf-8') as f:
            data = f.read()
        print("----- AI Pruning -----")
        completion = Doubao_32.chat.completions.create(
            # æŒ‡å®šæ‚¨åˆ›å»ºçš„æ–¹èˆŸæ¨ç†æ¥å…¥ç‚¹ IDï¼Œæ­¤å¤„å·²å¸®æ‚¨ä¿®æ”¹ä¸ºæ‚¨çš„æ¨ç†æ¥å…¥ç‚¹ ID
            model="doubao-1-5-pro-32k-250115",
            messages=[
                {"role": "system", "content": prtm},
                {"role": "user", "content": data},
            ],
             temperature=0.3
        )   
        _returndata = completion.choices[0].message.content
        _dicred = loads(_returndata)
        BlackList = [int(x) if isinstance(x, str) else x for x in (_dicred['DEL_IDX'] if 'DEL_IDX' in _dicred else [])]
        print(BlackList)
        #ç”Ÿæˆå‰ªåˆ‡æåçš„æ–‡ä»¶å’Œhtml
        _pathCleanCsv = self.MemPath + "/" + CSVCLEANED_FILENAME
        _pathHtmled= self.MemPath + "/" + HTMLED_FILENAME
        cutTreeNode(_pathCsv, _pathCleanCsv, BlackList)             #å¯¼å…¥æºæ–‡ä»¶åˆ‡æˆæ–°çš„

        GeneHtml(_pathCleanCsv,_pathHtmled)
    
    # [åŠŸèƒ½]æ•°æ®äºŒæ¬¡ç²‰ç­›å®¡æŸ¥
    def Category(self):
        _pathCleanCsv = self.MemPath + "/" + CSVCLEANED_FILENAME
        # è¯»å– CSV å¹¶è½¬æ¢ä¸º {Index: Url} å­—å…¸
        with open(_pathCleanCsv, mode='r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)  # è¿™ä¸€å¥ç”¨æ¥è·³è¿‡ç¬¬ä¸€è¡Œè¡¨å¤´ï¼ˆIndex, FatherInd...ï¼‰
            # ç›´æ¥å–ç¬¬0åˆ—åškeyï¼Œç¬¬5åˆ—åšvalue
            CLEANED_DIC = {row[0]: row[5] for row in reader}

        # æ‰“å°ç»“æœæŸ¥çœ‹
        #print(CLEANED_DIC)
        #input()
        # chuckså°±æ˜¯æ•°æ®åŒ…å—äº†
        chunks = packChunks(_pathCleanCsv,self.oriUrl)
        with open(DATA_BASE + '/' + "Get.txt",mode = 'r',encoding='utf-8') as f:
            prtm = f.read()
        
        FILE = []
        PAGE = []
        OTHER= []

        for i, chunk in enumerate(chunks):
            OKNoise()
            print("----- AI Categoring -----")
            completion = Doubao_32.chat.completions.create(
                # æŒ‡å®šæ‚¨åˆ›å»ºçš„æ–¹èˆŸæ¨ç†æ¥å…¥ç‚¹ IDï¼Œæ­¤å¤„å·²å¸®æ‚¨ä¿®æ”¹ä¸ºæ‚¨çš„æ¨ç†æ¥å…¥ç‚¹ ID
                model="doubao-1-5-pro-32k-250115",
                messages=[
                    {"role": "system", "content": prtm},
                    {"role": "user", "content": chunk},
                ],
            )
            dics = loads(completion.choices[0].message.content)

            FILE    += [{"Index":_k,"title":dics["FILE"][_k],"url":CLEANED_DIC[_k] if _k in CLEANED_DIC else "ERROR" ,"Type":"FILE"}  for _k in dics["FILE"].keys()] if "FILE" in dics else []               
            PAGE    += [{"Index":_k,"title":dics["PAGE"][_k],"url":CLEANED_DIC[_k] if _k in CLEANED_DIC else "ERROR" ,"Type":"PAGE"}  for _k in dics["PAGE"].keys()] if "PAGE" in dics else [] 
            OTHER   += [{"Index":_k,"title":dics["OTHER"][_k],"url":CLEANED_DIC[_k] if _k in CLEANED_DIC else "ERROR" ,"Type":"OTHER"} for _k in dics["OTHER"].keys()] if "OTHER" in dics else [] 
        CHECK_Noise()    
        #å­˜å‚¨æ•°æ®

        _pathSave1     = self.MemPath + "/"  + "categoryA.csv"
        _pathSave2     = self.MemPath + "/"  + "categoryB.csv"
        
        FILE += PAGE
        with open(_pathSave1, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ["Index", "title", "url", "Type"]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(FILE)
            
        with open(_pathSave2, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ["Index", "title", "url", "Type"]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(OTHER)
        

# é˜¶æ¢¯çŠ¶çˆ¬è¡Œè€…
class OverView_FloorMode():
    pass






#è§£å†³äººæœºéªŒè¯é—®é¢˜
def CookieGo(driver):
    """å¤„ç†Cookieå¼¹çª—ï¼Œå‘ç°äººæœºéªŒè¯åˆ™å›é€€"""
    page_source = driver.page_source
    # 1. æ£€æµ‹äººæœºéªŒè¯ (Captcha)
    captcha_signals = ["g-recaptcha", "hcaptcha", "captcha-delivery", "äººæœºéªŒè¯", "ç§ã¯ãƒ­ãƒœãƒƒãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“"]
    if any(sig in page_source for sig in captcha_signals):
        print("æ£€æµ‹åˆ°äººæœºéªŒè¯ï¼Œæ­£åœ¨å°è¯•è¿”å›ä¸Šä¸€çº§...")
        driver.back()
        return False

    # 2. è‡ªåŠ¨ç‚¹å‡»CookieåŒæ„æŒ‰é’®
    cookie_keywords = ["åŒæ„", "Accept", "Agree", "OK", "ã¯ã„", "æ‰¿è«¾"]
    try:
        buttons = driver.find_elements(By.TAG_NAME, "button")
        for btn in buttons:
            if any(k in btn.text for k in cookie_keywords):
                btn.click()
                print("\tç‚¹å‡»Cookie")
                break
    except: pass
    return True

def clean_url(url):
    """è§„èŒƒåŒ–URLï¼Œå»é™¤é”šç‚¹(#)å’Œæœ«å°¾æ–œæ ï¼Œé˜²æ­¢åŒä¸€ä¸ªé¡µé¢è¢«ç´¢å¼•ä¸¤æ¬¡"""
    parsed = urlparse(url)
    cleaned = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, parsed.query, ""))
    return cleaned.rstrip('/')


    #è·å¾—é¡µé¢æ‘˜è¦
def GetIntroduce(soup,md_text):
    """ç”Ÿæˆé¡µé¢ç®€å•æ€»ç»“ï¼šæå–æ ‡é¢˜å’Œå‰100ä¸ªæœ‰æ„ä¹‰çš„å­—ç¬¦"""
    title = soup.title.string if soup.title else "No Title"
    clean_text = re.sub(r'\s+', ' ', md_text).strip()
    summary = f"Title: {title} | Content: {clean_text[:100]}..."
    return summary


def create_Folder(path):
    OSmakedirs(path) if not OSisdir(path) else 0
  
    
#é¢åŒ…å±‘è·¯å¾„æ•æ‰
def GetBreadcrumbs(soup):
    container = soup.find(attrs={"typeof": "BreadcrumbList"}) or \
                soup.find(id=re.compile(r'breadcrumb|topicpath', re.I)) or \
                soup.find(class_=re.compile(r'breadcrumb|topicpath', re.I))
    if not container:
        return "None"

    raw_items = []

    for element in container.find_all(['span', 'li', 'a']):

        text = element.get_text(strip=True)
        if text and text not in ['>', '/', 'Â»', 'ï¼']:

            if not raw_items or text != raw_items[-1]:
                raw_items.append(text)

    clean_items = [re.sub(r'[ğŸ \s]', '', i) for i in raw_items]
    clean_items = [i for i in clean_items if i and i.upper() not in ['HOME', 'TOP', 'ğŸ ', 'é¦–é¡µ']]

    if clean_items:
        return " > ".join(clean_items)
    
    return "None"

#æ‰“å°æ—¥å¿—ç±»
class DebugPrinter():
    def __init__(self,WriteDiary = False) -> None:
        self._DEBNumber = 1
        self._DEF_ANIDELTA = 0.05
        self.DEBUG = True
        self.color = "\033[36m"
        
        self.willWrite = WriteDiary
        self.debugFilePath = 'NO.txt'

        # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ - ä¸å­˜åœ¨åˆ™åˆ›å»º
        OSmakedirs("_debug") if not OSisdir("_debug") else 0
        NAME = str(time.strftime('%Y-%m-%d %H:%M:%S',time.localtime())).replace('-','_').replace(':','_').replace(' ','')
        _ind = 0
        if WriteDiary:
            while True:
                if OPAexists("_debug/" + NAME + ('' if _ind == 0 else str(_ind)) + ".txt"): 
                    _ind += 1
                else:
                    _Fin = "_debug/" + NAME+ ('' if _ind == 0 else str(_ind)) + ".txt"
                    self.debugFilePath = _Fin
                    with open(_Fin, 'w', encoding='utf-8') as file:
                        file.write("[DEBUG]"+NAME+"\n");file.close()	
                    break
                

    #[åŠŸèƒ½] åŠ¨ç”»æµè¾“å‡º
    def DEBAnimaPrint(self,a,ends = '\n\n',_delta = None,coL = True):
        delta = self._DEF_ANIDELTA if _delta == None else _delta
        #coL = "\033[36m" if coL else "\033[37m"
        _col = self.color if coL else "\033[37m"
        print('\nã€'+str(self._DEBNumber)+'ã€‘\t',end='')
        for _a in str(a):
            time.sleep(delta); 
            _A = print( _col + _a + "\033[0m",end='') if self.DEBUG else 0
            stdout.flush()
        print('\a',end=ends);time.sleep(1);
        self._writeDiary('\nã€'+str(self._DEBNumber)+'ã€‘\t' + str(a) + ends)
        self._DEBNumber+=1;

    #[åŠŸèƒ½] ç›´æ¥è¾“å‡º       
    def DEBPrint(self,a,b = '' ,c = '',ends = '\n\n'):
        _a = print('\nã€'+str(self._DEBNumber)+'ã€‘\t',a,b,c,end=ends) if self.DEBUG else 0;
        self._writeDiary('\nã€'+str(self._DEBNumber)+'ã€‘\t' + str(a) + ends)
        self._DEBNumber+=1;
        Tsleep(0.1)

    #[åŠŸèƒ½] åŠ¨ç”»æµè¾“å‡º
    def _writeDiary(self,content = ''):
        if (not self.willWrite):
            print("EMPTY")
            return
        with open(self.debugFilePath, 'a', encoding='utf-8') as file:
            file.write(content);file.close()

    def reset(self,Sign = 1):
        self._DEBNumber = Sign





# å’Œè¿™ä¸ªç¨‹åºç»‘å®šçš„ DEBP
_DP = DebugPrinter(WriteDiary=True)
DEBPrint = _DP.DEBPrint
DEBAnimaPrint = _DP.DEBAnimaPrint





# é¢„å®šä¹‰éŸ³æ•ˆé€»è¾‘
def get_signal(t, effect):
    if effect == "success":      
        if t < 0.1:
            return 4 * math.sin(2 * math.pi * 987.77 * t)
        elif t < 1.0:
            decay = 1.0 - ((t - 0.1) / 0.9)
            return 2 * math.sin(2 * math.pi * 1318.51 * t) * decay
    elif effect == "danger":
        freq = 1000 + 400 * (1 if (t * 10) % 2 > 1 else -1)
        return 1.0 if 2*math.sin(2 * math.pi * freq * t) > 0 else -1.0
    elif effect == "error":
        freq = max(30, 200 - t * 170)
        noise = math.sin(2 * math.pi * freq * t) + 0.5 * math.sin(2 * math.pi * (freq*0.5) * t)
        return 1.0 if noise > 0 else -1.0
    elif effect == "progress":
        sub_t = t % 0.5
        if sub_t < 0.15:
            return math.sin(2 * math.pi * (880 + sub_t * 1500) * sub_t)
    return 0


#[åŠŸèƒ½]  æ’­æ”¾éŸ³é¢‘ (è·¨å¹³å°å…¼å®¹)
def play_effect(effect_type):
    # é Windows å¹³å°é™é»˜è·³è¿‡éŸ³æ•ˆ
    if platform.system() != 'Windows':
        return

    try:
        import winsound
        sample_rate, duration, amplitude = 44100, 1.0, 3600
        byte_io = io.BytesIO()
        with wave.open(byte_io, 'wb') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(sample_rate)
            num_samples = int(sample_rate * duration)
            samples = []
            for i in range(num_samples):
                val = get_signal(i / sample_rate, effect_type)
                samples.append(struct.pack('<h', int(val * amplitude)))
                if len(samples) > 1000:
                    wav_file.writeframes(b''.join(samples))
                    samples = []
            wav_file.writeframes(b''.join(samples))
        winsound.PlaySound(byte_io.getvalue(), winsound.SND_MEMORY)
    except ImportError:
        pass  # winsound ä¸å¯ç”¨æ—¶é™é»˜è·³è¿‡
  
# ç”Ÿæˆç¼©ç•¥ç‰ˆé“¾æ¥
def GetShortURL(base_url, target_url):
    #åˆ¤æ–­ target_url æ˜¯å¦å±äº base_url çš„æœ¬åŸŸã€‚
    #å¦‚æœæ˜¯ï¼Œè¿”å›ç›¸å¯¹è·¯å¾„ï¼ˆ/path/to/pageï¼‰ï¼›
    #å¦‚æœä¸æ˜¯ï¼ˆè·¨åŸŸæˆ–å¤–éƒ¨é“¾æ¥ï¼‰ï¼Œè¿”å›å®Œæ•´çš„ target_urlã€‚
    try:
        base_p = urlparse(base_url)
        target_p = urlparse(target_url)

        # æ ¸å¿ƒåˆ¤æ–­ï¼šæ¯”è¾ƒåŸŸå (netloc) 
        # ä¾‹å¦‚: www.chiba-u.ac.jp å’Œ www.chiba-u.ac.jp æ˜¯å¦ä¸€è‡´
        if base_p.netloc == target_p.netloc and base_p.scheme == target_p.scheme:
            # ç»„è£…ç›¸å¯¹è·¯å¾„ï¼špath + params + query + fragment
            # æ³¨æ„ï¼šå¦‚æœ path ä¸ºç©ºï¼Œè‡³å°‘è¿”å›ä¸€ä¸ª /
            short_path = target_p.path if target_p.path else "/"
            if target_p.query:
                short_path += "?" + target_p.query
            if target_p.fragment:
                short_path += "#" + target_p.fragment
                
            return base_url if "/" == short_path else short_path
        else:
            # è·¨åŸŸäº†ï¼Œå¿…é¡»è¿”å›å®Œæ•´åœ°å€ï¼Œå¦åˆ™ AI ä¼šæ‹¼é”™
            return target_url
    except Exception:
        # å‘ç”Ÿæ„å¤–ï¼ˆå¦‚æ ¼å¼æå…¶ç¦»å¥‡ï¼‰ï¼Œè¿”å›åŸæ ·ä»¥ä¿å®‰å…¨
        return target_url
  
    
# ç”ŸæˆHTMLé“¾æ¥
def GeneHtml(csv_file, output_name="university_network.html"):
    # 1. è¯»å–å¹¶å¤„ç†æ•°æ®
    data = []
    try:
        with open(csv_file, mode='r', encoding='utf-8-sig') as f:
            # ä½¿ç”¨ DictReader è‡ªåŠ¨å°†è¡¨å¤´ä½œä¸º Key
            reader = csv.DictReader(f)
            for row in reader:
                data.append(row)
    except Exception as e:
        print(f"[-] è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return

    if not data:
        print("[-] CSV æ–‡ä»¶ä¸ºç©º")
        return

    # åˆ›å»ºä¸€ä¸ªç´¢å¼•é›†ï¼Œç”¨äºå¿«é€Ÿæ£€æŸ¥ FatherIndex æ˜¯å¦å­˜åœ¨ (ä»£æ›¿ pandas çš„ .values)
    all_indices = {int(row['Index']) for row in data}
    
    nodes = []
    edges = []

    # ğŸ¨ æ›´åŠ ç²¾ç¾çš„é…è‰²æ–¹æ¡ˆ (å®šä¹‰åœ¨å¾ªç¯å¤–)
    COLOR_MAP = {
        -1: {"bg": "#2f3640", "border": "#1e272e"}, # æ ¹æ ¹
        0:  {"bg": "#ff6b6b", "border": "#ee5253"}, # æ ¹èŠ‚ç‚¹ (çº¢è‰²)
        1:  {"bg": "#feca57", "border": "#ff9f43"}, # ä¸€çº§ (æ©™é»„)
        2:  {"bg": "#1dd1a1", "border": "#10ac84"}, # äºŒçº§ (ç¿ ç»¿)
        3:  {"bg": "#48dbfb", "border": "#0abde3"}, # ä¸‰çº§ (å¤©è“)
    }

    # 2. æ„é€ èŠ‚ç‚¹ä¸è¾¹
    for row in data:
        idx = int(row['Index'])
        father_idx = int(row['FatherIndex'])
        title = str(row['title']).replace('"', '\\"')
        url = str(row['Url'])
        breadcrumb = str(row['Breadcrumb']).replace('"', '\\"')
        depth = int(row['Depth'])

        # --- èŠ‚ç‚¹å¤„ç† ---
        style = COLOR_MAP.get(depth, {"bg": "#c8d6e5", "border": "#8395a7"})
        
        nodes.append({
            "id": idx,
            "label": (title[:12] + "..") if len(title) > 12 else title,
            "fullTitle": title,
            "url": url,
            "breadcrumb": breadcrumb,
            "color": {
                "background": style["bg"],
                "border": style["border"],
                "highlight": {"background": "#ffffff", "border": style["bg"]},
                "hover": {"background": "#ffffff", "border": style["bg"]}
            },
            "borderWidth": 3,
            "shape": "dot",
            "size": 30 if depth == 0 else (22 if depth == 1 else 15),
            "shadow": {"enabled": True, "color": "rgba(0,0,0,0.2)", "size": 10, "x": 5, "y": 5}
        })

        # --- è¾¹å¤„ç† ---
        if father_idx != -1 and father_idx in all_indices:
            edges.append({
                "from": father_idx, 
                "to": idx,
                "color": {"color": "#a4b0be", "highlight": "#54a0ff"},
                "width": 1.5,
                "arrows": {"to": {"enabled": True, "scaleFactor": 0.4}}
            })

    # 3. HTML æ¨¡æ¿ (é€»è¾‘ä¿æŒä¸å˜)
    html_template = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>å¤§å­¦å…¥è¯•çŸ¥è¯†å›¾è°±</title>
    <script type="text/javascript" src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
    <style>
        body, html {{ margin: 0; padding: 0; width: 100%; height: 100%; overflow: hidden; font-family: 'Segoe UI', sans-serif; }}
        #mynetwork {{ width: 100%; height: 100vh; background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%); }}
        #loader {{
            position: fixed; top: 0; left: 0; width: 100%; height: 100%;
            background: rgba(255,255,255,0.9);
            display: flex; flex-direction: column; justify-content: center; align-items: center;
            z-index: 9999;
        }}
        .progress-container {{ width: 300px; height: 10px; background: #eee; border-radius: 5px; overflow: hidden; margin-top: 20px; }}
        #progress-bar {{ width: 0%; height: 100%; background: #54a0ff; transition: width 0.1s; }}
        #progress-text {{ font-size: 14px; color: #576574; font-weight: bold; }}
        .custom-tooltip {{
            position: absolute; visibility: hidden;
            background: rgba(255, 255, 255, 0.9);
            backdrop-filter: blur(5px);
            border-left: 6px solid #54a0ff;
            border-radius: 12px; padding: 18px;
            box-shadow: 0 15px 35px rgba(0,0,0,0.15);
            pointer-events: none; z-index: 1000; max-width: 320px;
        }}
        .t-path {{ font-size: 11px; color: #8395a7; text-transform: uppercase; letter-spacing: 1px; margin-bottom: 8px; }}
        .t-id {{ font-weight: bold; color: #54a0ff; font-size: 12px; margin-bottom: 5px; }}
        .t-title {{ font-size: 16px; color: #2d3436; line-height: 1.5; font-weight: 600; }}
        .t-footer {{ margin-top: 12px; font-size: 11px; color: #ee5253; text-align: right; border-top: 1px solid rgba(0,0,0,0.05); padding-top: 10px; font-style: italic; }}
    </style>
</head>
<body>
    <div id="loader">
        <div id="progress-text">æ­£åœ¨è®¡ç®—èŠ‚ç‚¹å¸ƒå±€... 0%</div>
        <div class="progress-container"><div id="progress-bar"></div></div>
    </div>
    <div id="tooltip" class="custom-tooltip"></div>
    <div id="mynetwork"></div>

    <script type="text/javascript">
        var nodes = new vis.DataSet({json.dumps(nodes)});
        var edges = new vis.DataSet({json.dumps(edges)});

        var container = document.getElementById('mynetwork');
        var tooltip = document.getElementById('tooltip');
        var progressBar = document.getElementById('progress-bar');
        var progressText = document.getElementById('progress-text');
        var loader = document.getElementById('loader');
        
        var data = {{ nodes: nodes, edges: edges }};
        var options = {{
            physics: {{
                forceAtlas2Based: {{ gravConstant: -120, centralGravity: 0.01, springLength: 120, damping: 0.4 }},
                solver: 'forceAtlas2Based',
                stabilization: {{ iterations: 200 }}
            }},
            interaction: {{ hover: true, tooltipDelay: 0 }},
            nodes: {{ font: {{ face: 'Segoe UI', size: 14, strokeWidth: 4, strokeColor: '#ffffff' }} }}
        }};

        var network = new vis.Network(container, data, options);

        network.on("stabilizationProgress", function(params) {{
            var progress = Math.round((params.iterations / params.total) * 100);
            progressBar.style.width = progress + '%';
            progressText.innerText = 'æ­£åœ¨è®¡ç®—èŠ‚ç‚¹å¸ƒå±€... ' + progress + '%';
        }});

        network.once("stabilizationIterationsDone", function() {{
            loader.style.opacity = '0';
            setTimeout(() => loader.style.display = 'none', 500);
        }});

        network.on("hoverNode", function (params) {{
            var node = nodes.get(params.node);
            tooltip.innerHTML = `
                <div class="t-path">${{node.breadcrumb}}</div>
                <div class="t-id">NODE INDEX: ${{node.id}}</div>
                <div class="t-title">${{node.fullTitle}}</div>
                <div class="t-footer">ğŸ–±ï¸ åŒå‡»è·³è½¬è‡³é“¾æ¥</div>
            `;
            tooltip.style.visibility = "visible";
        }});

        network.on("blurNode", function () {{ tooltip.style.visibility = "hidden"; }});

        container.addEventListener('mousemove', function(e) {{
            tooltip.style.left = (e.pageX + 20) + 'px';
            tooltip.style.top = (e.pageY + 20) + 'px';
        }});

        network.on("doubleClick", function (params) {{
            if (params.nodes.length > 0) {{
                var node = nodes.get(params.nodes[0]);
                if (node.url && node.url !== 'None') {{ window.open(node.url, '_blank'); }}
            }}
        }});
    </script>
</body>
</html>
    """

    with open(output_name, "w", encoding="utf-8") as f:
        f.write(html_template)
    print(f"[+] è½¬æ¢æˆåŠŸï¼Œå·²ç”Ÿæˆ: {output_name}")



def GeneMD(INPUT_CSV,OUTPUT_MD,BASE_URL,IgnoreChile:bool = False):
    # 1. å…¨å±€æ•°æ®ç´¢å¼•åŒ– (ä¸ºäº†èƒ½éšæ—¶æŸ¥å‡ºä»»ä½•èŠ‚ç‚¹çš„å®Œæ•´ä¿¡æ¯)
    all_nodes = {}  # { Index: {å®Œæ•´æ•°æ®} }
    buckets = {}    # { FatherIndex: [å­èŠ‚ç‚¹Indexåˆ—è¡¨] }
    
    print(f"âŒ› æ­£åœ¨å…¨é‡è§£ææ•°æ®...")
    with open(INPUT_CSV, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            idx = row['Index']
            f_idx = row['FatherIndex']
            
            # å­˜å‚¨èŠ‚ç‚¹å®Œæ•´æ•°æ®
            all_nodes[idx] = row
            
            # åˆ†æ¡¶
            if f_idx not in buckets:
                buckets[f_idx] = []
            buckets[f_idx].append(idx)

    # 2. æ’åºï¼ˆæŒ‰å­èŠ‚ç‚¹æ•°é‡é™åºï¼‰
    sorted_f_indices = sorted(buckets.keys(), key=lambda x: len(buckets[x]), reverse=True)

    # 3. ç”Ÿæˆè¾“å‡ºå†…å®¹
    output = []
    output.append(f"BASE_URL: {BASE_URL}")
    output.append("FORMAT_SCHEMA: Index | FatherIndex | Short_URL | Breadcrumb | Title\n")
    output.append("--- START OF STRUCTURED DATA ---\n")

    mis_f_indices = [] # å­˜æ”¾å°è§„æ¨¡ç¾¤ç»„çš„çˆ¶ID

    def format_line(node_idx):
        """ç»Ÿä¸€çš„è¡Œæ ¼å¼åŒ–å·¥å…·"""
        node = all_nodes.get(str(node_idx))
        if not node: return f"{node_idx} | Unknown Node Data"
        
        idx = node['Index']
        f_id = node['FatherIndex']
        url = GetShortURL(BASE_URL,node['Url'])
        bc = node.get('Breadcrumb', 'None')
        title = node.get('title', 'None')
        return f"{idx} | {f_id} | {url} | {bc} | {title}"

    # ç¬¬ä¸€éƒ¨åˆ†ï¼šå¤§å‹ç¾¤ç»„ (å­èŠ‚ç‚¹ > 2)
    for f_idx in sorted_f_indices:
        child_indices = buckets[f_idx]
        if len(child_indices) > 2:
            output.append(f"# [CLUSTER_GROUP_START: FatherNode {f_idx}]")
            
            # ã€å…³é”®ä¿®æ­£ã€‘: å…ˆæ‰“å°çˆ¶èŠ‚ç‚¹æœ¬èº«çš„å®Œæ•´ä¿¡æ¯
            if f_idx in all_nodes:
                output.append(f"FatherNode -> {format_line(f_idx)}")
                output.append("-" * 25) # å°åˆ†éš”çº¿è¡¨ç¤ºçˆ¶å­å…³ç³»
            else:
                output.append(f"FatherNode -> {f_idx} | -1 | ROOT_OR_EXTERNAL | None | SEED_PAGE")

            # æ‰“å°å­èŠ‚ç‚¹
            for c_idx in child_indices:
                output.append(format_line(c_idx))
            output.append(f"# [CLUSTER_GROUP_END]\n")
        else:
            mis_f_indices.append(f_idx)
    if not IgnoreChile:
        # ç¬¬äºŒéƒ¨åˆ†ï¼šæ•£ç¢èŠ‚ç‚¹ï¼ˆå•èŠ‚ç‚¹/åŒèŠ‚ç‚¹ï¼‰
        if mis_f_indices:
            output.append("\n" + "="*60)
            output.append("# REMARK_FOR_AI: The following are scattered single/double nodes.")
            output.append("# FORMAT REMAINS THE SAME. Father information is embedded in each line.")
            output.append("# SECTION: MISC_LINKS_FLAT_LIST")
            output.append("="*60 + "\n")
        
            for f_idx in mis_f_indices:
                # å³ä½¿æ˜¯æ•£ç¢èŠ‚ç‚¹ï¼Œä¹Ÿè¦æŠŠå®ƒä»¬å±äºå“ªä¸ªâ€œçˆ¸çˆ¸â€æ‰“å°æ¸…æ¥š
                for c_idx in buckets[f_idx]:
                    output.append(format_line(c_idx))

    # 4. å†™å…¥æ–‡ä»¶
    with open(OUTPUT_MD, 'w', encoding='utf-8') as f:
        f.write("\n".join(output))

    DEBPrint(f"\tä»»åŠ¡OKï¼")
    DEBPrint(f"\tç²¾ç‚¼æŠ¥è¡¨å·²ä¿å­˜: {OUTPUT_MD}")
    DEBPrint(f"\tèŠ‚ç‚¹æ€»æ•°: {len(all_nodes)}")
    
warningNoise = lambda : play_effect("danger")       #è·³è¿‡äº†åº—é“º
OKNoise      = lambda : play_effect("progress")     #ç¨‹åºé¡ºåˆ©è¿›è¡Œ - å¦‚è¿›å…¥é¡µé¢ï¼Œå¼€å§‹çˆ¬å–æ•°æ®,ä¼‘æ¯
CHECK_Noise   = lambda : play_effect("success")      #æš—ç¤ºä¹¦ç±åˆ°æ‰‹ 
ERROR_Noise   = lambda : play_effect("error")        #Seleniumå‡ºé”™ æˆ–è€…ç™»å½•å‡ºé”™-è¿™ç§æ—¶å€™å¯èƒ½ä¼šå¯¼è‡´ç¨‹åºå…³é—­ å°±ä¼šæŠ¥é”™è¿™ä¸ª



#è¿™é‡Œæˆ‘ä¼˜åŒ–ä¸¤æ¬¡äº†ï¼Œå¼ºè¡Œç¼©å°äº†å¾ˆå¤šå†…å­˜

def Easied(input_file, output_file,max_children=3):
    """
    å¼ºåˆ¶ç¡¬é™æŠ½æ ·ï¼šç¡®ä¿è¾“å‡ºç»“æœä¸­ï¼Œä»»ä½•çˆ¶èŠ‚ç‚¹åœ¨ç¬¬äºŒåˆ—çš„å‡ºç°æ¬¡æ•°ç»å¯¹ä¸è¶…è¿‡ max_childrenã€‚
    """
    all_nodes = {}
    children_map = defaultdict(list)
    header = None
    
    IDX_COL = 0
    PIDX_COL = 1

    # 1. ç¬¬ä¸€æ¬¡éå†ï¼šæ„å»ºå†…å­˜ç´¢å¼•
    with open(input_file, mode='r', encoding='utf-8-sig') as f:
        reader = csv.reader(f)
        header = next(reader)
        for row in reader:
            if not row: continue
            idx = row[IDX_COL]
            pidx = row[PIDX_COL]
            all_nodes[idx] = row
            if pidx and pidx != "-1":
                children_map[pidx].append(row)

    final_rows_dict = {} # ä½¿ç”¨å­—å…¸ç¡®ä¿å”¯ä¸€æ€§ {idx: row}

    # 2. éå†æ‰€æœ‰æ‹¥æœ‰å­èŠ‚ç‚¹çš„çˆ¶çº§
    for pidx, children in children_map.items():
        # å¼ºåˆ¶ç­–ç•¥ A: å¦‚æœçˆ¶èŠ‚ç‚¹æœ¬èº«ä¸åœ¨ç»“æœé›†ä¸­ï¼Œå…ˆæŠŠå®ƒåŠ è¿›å»ï¼ˆä½œä¸ºèƒŒæ™¯å‚è€ƒï¼‰
        # æ³¨æ„ï¼šçˆ¶èŠ‚ç‚¹ row è‡ªå·±çš„ PIDX æ˜¯å®ƒæ›´ä¸Šä¸€çº§çš„ï¼Œæ‰€ä»¥ä¸å ç”¨å½“å‰ pidx çš„è®¡æ•°
        if pidx in all_nodes and pidx not in final_rows_dict:
            final_rows_dict[pidx] = all_nodes[pidx]

        # å¼ºåˆ¶ç­–ç•¥ B: ä¸¥æ ¼é™åˆ¶å­èŠ‚ç‚¹æ•°é‡
        # æ— è®ºåŸæœ¬æœ‰å¤šå°‘ï¼Œåªå–å‰ max_children ä¸ª
        sampled_children = children[:max_children]
        
        for child_row in sampled_children:
            c_idx = child_row[IDX_COL]
            final_rows_dict[c_idx] = child_row

    # 3. ç»“æœè½¬æ¢ä¸å®‰å…¨æ€§æ ¡éªŒ
    final_data = list(final_rows_dict.values())
    
    # ç‰©ç†éªŒè¯ï¼ˆé˜²æ­¢é€»è¾‘æ¼æ´ï¼‰
    counts = defaultdict(int)
    safe_data = []
    # é‡æ–°åŠ å…¥è¡¨å¤´
    safe_data.append(header)
    
    # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†åº”å¯¹ä½ è¯´çš„â€œç›´ç™½è¦æ±‚â€ï¼šç¬¬äºŒåˆ—ç›¸åŒINDEXä¸å¾—è¶…è¿‡5ä¸ªï¼ˆè¿™é‡Œé™åˆ¶ä¸º3ï¼‰
    for row in final_data:
        p_val = row[PIDX_COL]
        if p_val == "-1" or p_val == "":
            safe_data.append(row)
            continue
            
        if counts[p_val] < max_children:
            safe_data.append(row)
            counts[p_val] += 1
        else:
            # è¶…è¿‡äº†ç¡¬é™ï¼Œè¯¥è¡Œå³ä½¿è¢«é€‰ä¸­ä¹Ÿè¦èˆå¼ƒï¼ˆé™¤éå®ƒæ˜¯å…¶ä»–äººçš„çˆ¶èŠ‚ç‚¹ï¼Œä½†åœ¨è¿™ä¸€æ­¥æˆ‘ä»¬åªçœ‹å®ƒä½œä¸ºå­èŠ‚ç‚¹çš„æƒ…å†µï¼‰
            pass

    # 4. å¯¼å‡º
    with open(output_file, mode='w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerows(safe_data)

    DEBPrint(f"--- å¼ºåŠ›æŠ½æ ·å®Œæˆ ---")
    DEBPrint(f"åŸå§‹æ•°æ®: {len(all_nodes)} è¡Œ")
    DEBPrint(f"æŠ½æ ·å(å«è¡¨å¤´): {len(safe_data)} è¡Œ")
    DEBPrint(f"å¼ºåˆ¶æ ‡å‡†: ç¬¬äºŒåˆ—(ParentIndex)é‡å¤ä¸Šé™ = {max_children}")
    
    return safe_data

# æ ¹æ®å‰ªæå†…å®¹ï¼Œåˆ‡æ‰æ— ç”¨éƒ¨åˆ†
def cutTreeNode(input_csv, output_csv, blacklist_indices):
    """
    æ ¹æ®AIè¿”å›çš„é»‘åå•ï¼Œç§»é™¤å¯¹åº”çš„å­èŠ‚ç‚¹ã€‚
    :param input_csv: åŸå§‹çš„65kbå…¨é‡æ•°æ®CSV
    :param output_csv: æ¸…æ´—åçš„ç²¾ç®€CSV
    :param blacklist_indices: AIåˆ¤æ–­ä¸ºæ— æ•ˆçš„çˆ¶èŠ‚ç‚¹INDEXåˆ—è¡¨
    """
    # å°†åˆ—è¡¨è½¬æ¢ä¸º setï¼Œæé«˜æŸ¥æ‰¾æ•ˆç‡ (O(1) å¤æ‚åº¦)
    black_set = set(str(i) for i in blacklist_indices)
    
    cleaned_rows = []
    removed_count = 0
    
    try:
        with open(input_csv, mode='r', encoding='utf-8-sig') as f:
            reader = csv.reader(f)
            header = next(reader) # è·å–è¡¨å¤´
            cleaned_rows.append(header)
            
            # å‡è®¾ CSV ç»“æ„ï¼š0: INDEX, 1: PARENT_INDEX
            # å¦‚æœä½ çš„åˆ—é¡ºåºä¸åŒï¼Œè¯·ä¿®æ”¹ä¸‹é¢çš„ç´¢å¼•å€¼
            IDX_COL = 0
            PIDX_COL = 1
            
            for row in reader:
                # å®¹é”™å¤„ç†ï¼šè·³è¿‡ç©ºè¡Œ
                if not row: continue
                
                current_idx = row[IDX_COL]
                parent_idx = row[PIDX_COL]
                
                # åˆ¤å®šé€»è¾‘ï¼š
                # å¦‚æœå½“å‰èŠ‚ç‚¹çš„ PARENT_INDEX åœ¨é»‘åå•é‡Œï¼Œè¯´æ˜å®ƒæ˜¯â€œå¤§å¤´å™ªéŸ³â€çš„å­èŠ‚ç‚¹ï¼Œè·³è¿‡ï¼ˆåˆ é™¤ï¼‰
                if parent_idx in black_set:
                    removed_count += 1
                    continue
                
                # å¦åˆ™ä¿ç•™
                cleaned_rows.append(row)
                
        # å†™å…¥æ–° CSV
        with open(output_csv, mode='w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(cleaned_rows)
            
        DEBPrint(f"--- æ•°æ®æ¸…æ´—å®Œæˆ ---")
        DEBPrint(f"åŸå§‹è®°å½•æ€»æ•°ï¼ˆå«è¡¨å¤´ï¼‰: {len(cleaned_rows) + removed_count}")
        DEBPrint(f"æˆåŠŸç§»é™¤å­èŠ‚ç‚¹æ•°é‡: {removed_count}")
        DEBPrint(f"ä¿ç•™èŠ‚ç‚¹æ•°é‡: {len(cleaned_rows)}")
        DEBPrint(f"æ¸…æ´—åçš„æ–‡ä»¶å·²ä¿å­˜è‡³: {output_csv}")

    except FileNotFoundError:
        DEBPrint(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {input_csv}ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")


# æŠŠæ•°æ®æ‰“åŒ…æˆå¤šä¸ªåŒ…å‘é€ç»™ai
def packChunks(input_csv,base_url):
# 1. é…ç½®åˆ—ç´¢å¼•
    IDX_COL = 0
    PIDX_COL = 1
    TITLE_COL = 3
    BREAD_COL = 4
    URL_COL = 5
    FTITLE_COL = 6
    
    # å®šä¹‰æœ€å¤§å­—èŠ‚æ•° (é¢„ç•™ 500 å­—èŠ‚ç»™ Group ç»“æŸç¬¦ç­‰ç¼“å†²)
    MAX_BYTES = int(PACK_MAX_SIZE * 1024) 
    SAFE_LIMIT = MAX_BYTES - 500 

    # 2. è¯»å–æ•°æ® (ä¿æŒä¸å˜)
    all_nodes = {}
    parent_to_children = defaultdict(list)
    
    with open(input_csv, mode='r', encoding='utf-8-sig') as f:
        reader = csv.reader(f)
        header = next(reader)
        for row in reader:
            if not row: continue
            all_nodes[row[IDX_COL]] = row
            if row[PIDX_COL] != "-1":
                parent_to_children[row[PIDX_COL]].append(row)

    # 3. å®šä¹‰å•è¡Œæ–‡æœ¬åŒ–å‡½æ•° (ä¿æŒä¸å˜)
    def row_to_text(row):
        return f"{row[IDX_COL]} | {row[FTITLE_COL]} | {row[BREAD_COL]} | {row[TITLE_COL]} | {GetShortURL(base_url,row[URL_COL])}\n"

    # ================= ä¿®æ”¹é‡ç‚¹å¼€å§‹ =================
    # 4. æŒ‰å®¶æ—æ‰“åŒ…æ–‡æœ¬ (æ–°å¢ï¼šå¤§â€œå®¶æ—â€æ‹†åˆ†é€»è¾‘)
    families = []
    
    for pidx, children in parent_to_children.items():
        # å‡†å¤‡çˆ¶èŠ‚ç‚¹ä¿¡æ¯å¤´
        header_text = ""
        if pidx in all_nodes:
            parent_row = all_nodes[pidx]
            header_text = f"-- GROUP START (Father: {pidx}) --\n"
            header_text += "FATHER -> " + row_to_text(parent_row)
        else:
            header_text = f"-- GROUP START (Father: {pidx} - Missing) --\n"

        footer_text = "-- GROUP END --\n\n"
        
        # å½“å‰æ­£åœ¨æ„å»ºçš„æ–‡æœ¬å—
        current_fam_text = header_text
        
        for child in children:
            child_line = "  CHILD -> " + row_to_text(child)
            
            # é¢„åˆ¤åŠ å…¥è¿™è¡Œåæ˜¯å¦ä¼šè¶…é™
            # æ³¨æ„ï¼šå¦‚æœè¿™æ˜¯æ–°èµ·çš„ä¸€ä¸ªå—ï¼Œå¿…é¡»è‡³å°‘æ”¾ä¸€è¡Œï¼Œå¦åˆ™ä¼šæ­»å¾ªç¯
            current_len = len(current_fam_text.encode('utf-8'))
            child_len = len(child_line.encode('utf-8'))
            
            # å¦‚æœåŠ ä¸Šè¿™ä¸€è¡Œå­èŠ‚ç‚¹ + ç»“å°¾ç¬¦ ä¼šè¶…è¿‡å®‰å…¨é™åˆ¶
            if current_len + child_len + len(footer_text.encode('utf-8')) > SAFE_LIMIT:
                # 1. å°å­˜å½“å‰å—
                current_fam_text += footer_text
                families.append(current_fam_text)
                
                # 2. å¼€å¯æ–°å— (é‡è¦ï¼šä¸ºäº† AI ä¸Šä¸‹æ–‡ï¼Œæ–°å—å¿…é¡»å†æ¬¡åŒ…å«çˆ¶èŠ‚ç‚¹å¤´ä¿¡æ¯)
                current_fam_text = header_text + child_line
            else:
                # æ²¡è¶…é™ï¼ŒåŠ å…¥å½“å‰å—
                current_fam_text += child_line
        
        # å¾ªç¯ç»“æŸï¼ŒæŠŠæœ€åå‰©ä½™çš„éƒ¨åˆ†åŠ ä¸Šç»“å°¾å¹¶ä¿å­˜
        current_fam_text += footer_text
        families.append(current_fam_text)

    # 5. åˆ†å—é€»è¾‘ (é€»è¾‘ç®€åŒ–ï¼Œå› ä¸º Step 4 å·²ç»ä¿è¯äº†å•ä¸ª family ä¸ä¼šè¶…é™)
    chunks = []
    current_chunk = ""

    for fam_text in families:
        # æ£€æŸ¥åŠ å…¥å½“å‰å®¶æ—åæ˜¯å¦è¶…é™
        potential_size = len((current_chunk + fam_text).encode('utf-8'))
        
        if potential_size <= MAX_BYTES:
            current_chunk += fam_text
        else:
            # å¦‚æœ current_chunk æœ‰å†…å®¹ï¼Œå…ˆä¿å­˜
            if current_chunk:
                chunks.append(current_chunk)
            # å¼€å¯æ–°çš„ chunk
            current_chunk = fam_text
    
    if current_chunk:
        chunks.append(current_chunk)
    # ================= ä¿®æ”¹é‡ç‚¹ç»“æŸ =================

    # 6. æ‰“å°åˆ†å—ç»“æœæŠ¥å‘Š (ä¿æŒä¸å˜)

    DEBPrint(f"åˆ†å—æŠ¥å‘Š:")
    DEBPrint(f"æ€»è®¡ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µæ•°: {len(families)}")
    DEBPrint(f"æœ€ç»ˆåˆ‡åˆ†ä¸º AI æ‰¹æ¬¡æ•°: {len(chunks)}")
    print("-" * 30)
    
    for i, chunk in enumerate(chunks):
        size_kb = len(chunk.encode('utf-8')) / 1024
        father_count = chunk.count("FATHER ->")
        child_count = chunk.count("CHILD ->")
        
        DEBPrint(f"ã€Batch {i+1}ã€‘")
        DEBPrint(f"  - æ•°æ®å¤§å°: {size_kb:.2f} KB")
        DEBPrint(f"  - èŠ‚ç‚¹æ•°é‡: çˆ¶çº§ {father_count} | å­çº§ {child_count}")
        print("-" * 50)

    return chunks
